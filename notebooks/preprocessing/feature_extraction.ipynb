{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction - Run Once, Use Everywhere\n",
    "\n",
    "This notebook extracts all features (physiology, behavior, gaze) from preprocessing results and saves them to a pickle file.\n",
    "\n",
    "**Parameterized**: Set `TIMEFRAME` to 'PRE' or 'POST' for pre-decision or post-decision analysis.\n",
    "\n",
    "**Run this notebook ONCE after preprocessing completes.**\n",
    "\n",
    "All other model notebooks will load the saved features instead of re-extracting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FEATURE EXTRACTION: POST-DECISION PERIOD\n",
      "Time window: 0.0 to 2.0 seconds\n",
      "======================================================================\n",
      "\n",
      "Feature extraction started: 2026-01-11 07:33:46\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION: Set timeframe for analysis\n",
    "# ============================================================================\n",
    "TIMEFRAME = 'POST'  # Options: 'PRE', 'POST' \n",
    "# ============================================================================\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import re\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set time window based on TIMEFRAME\n",
    "if TIMEFRAME == 'PRE':\n",
    "    TIME_WINDOW = (-2.0, 0.0)  # PRE-decision: -2 to 0 seconds before submit\n",
    "    SUFFIX = '_pre'\n",
    "elif TIMEFRAME == 'POST':\n",
    "    TIME_WINDOW = (0.0, 2.0)   # POST-decision: 0 to 2 seconds after submit\n",
    "    SUFFIX = '_post'\n",
    "else:\n",
    "    raise ValueError(\"TIMEFRAME must be 'PRE' or 'POST'\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"FEATURE EXTRACTION: {TIMEFRAME}-DECISION PERIOD\")\n",
    "print(f\"Time window: {TIME_WINDOW[0]} to {TIME_WINDOW[1]} seconds\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "print(f\"Feature extraction started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract Physiology and Behavior Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 99 preprocessing files\n",
      "Baseline correction method: t3_stable_pre_decision\n"
     ]
    }
   ],
   "source": [
    "preprocessing_dir = Path('../../data/results/preprocessing_outputs/preprocessing')\n",
    "preprocessing_files = sorted(preprocessing_dir.glob('preprocessing_*.json'))\n",
    "raw_dir = Path('../../data/raw/json')\n",
    "baseline_method = 't3_stable_pre_decision'\n",
    "\n",
    "print(f\"Found {len(preprocessing_files)} preprocessing files\")\n",
    "print(f\"Baseline correction method: {baseline_method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing subject: 0727_1400_539136F ✓ 0 trials\n",
      "\n",
      "Processing subject: 0727_1400_A6I5HI6 ✓ 0 trials\n",
      "\n",
      "Processing subject: 0806_1000_539136F ✓ 118 trials\n",
      "\n",
      "Processing subject: 0806_1000_U9TEJGM ✓ 131 trials\n",
      "\n",
      "Processing subject: 0811_1000_4LI8GO7 ✓ 121 trials\n",
      "\n",
      "Processing subject: 0811_1000_539136F ✓ 130 trials\n",
      "\n",
      "Processing subject: 0811_1000_U9TEJGM ✓ 131 trials\n",
      "\n",
      "Processing subject: 0813_1000_539136F ✓ 129 trials\n",
      "\n",
      "Processing subject: 0813_1000_9M4VCHG ✓ 123 trials\n",
      "\n",
      "Processing subject: 0813_1000_U9TEJGM ✓ 128 trials\n",
      "\n",
      "Processing subject: 0813_1600_539136F ✓ 124 trials\n",
      "\n",
      "Processing subject: 0813_1600_9M4VCHG ✓ 130 trials\n",
      "\n",
      "Processing subject: 0813_1600_U9TEJGM ✓ 123 trials\n",
      "\n",
      "Processing subject: 0816_1400_539136F ✓ 129 trials\n",
      "\n",
      "Processing subject: 0816_1400_9M4VCHG ✓ 124 trials\n",
      "\n",
      "Processing subject: 0816_1400_U9TEJGM ✓ 125 trials\n",
      "\n",
      "Processing subject: 0817_1000_539136F ✓ 131 trials\n",
      "\n",
      "Processing subject: 0817_1000_9M4VCHG ✓ 131 trials\n",
      "\n",
      "Processing subject: 0817_1000_U9TEJGM ✓ 119 trials\n",
      "\n",
      "Processing subject: 0818_1600_539136F ✓ 132 trials\n",
      "\n",
      "Processing subject: 0818_1600_9M4VCHG ✓ 90 trials\n",
      "\n",
      "Processing subject: 0818_1600_U9TEJGM ✓ 132 trials\n",
      "\n",
      "Processing subject: 0819_1400_539136F ✓ 129 trials\n",
      "\n",
      "Processing subject: 0819_1400_9M4VCHG ✓ 103 trials\n",
      "\n",
      "Processing subject: 0819_1400_U9TEJGM ✓ 129 trials\n",
      "\n",
      "Processing subject: 0823_1400_539136F ✓ 131 trials\n",
      "\n",
      "Processing subject: 0823_1400_9M4VCHG ✓ 132 trials\n",
      "\n",
      "Processing subject: 0823_1400_U9TEJGM ✓ 123 trials\n",
      "\n",
      "Processing subject: 0824_1000_539136F ✓ 131 trials\n",
      "\n",
      "Processing subject: 0824_1000_9M4VCHG ✓ 107 trials\n",
      "\n",
      "Processing subject: 0824_1000_U9TEJGM ✓ 114 trials\n",
      "\n",
      "Processing subject: 0824_1600_539136F ✓ 131 trials\n",
      "\n",
      "Processing subject: 0824_1600_9M4VCHG ✓ 128 trials\n",
      "\n",
      "Processing subject: 0824_1600_U9TEJGM ✓ 121 trials\n",
      "\n",
      "Processing subject: 0825_1000_539136F ✓ 37 trials\n",
      "\n",
      "Processing subject: 0825_1000_9M4VCHG ✓ 9 trials\n",
      "\n",
      "Processing subject: 0825_1300_539136F ✓ 124 trials\n",
      "\n",
      "Processing subject: 0825_1300_9M4VCHG ✓ 119 trials\n",
      "\n",
      "Processing subject: 0825_1300_U9TEJGM ✓ 89 trials\n",
      "\n",
      "Processing subject: 0826_1000_539136F ✓ 132 trials\n",
      "\n",
      "Processing subject: 0826_1000_9M4VCHG ✓ 127 trials\n",
      "\n",
      "Processing subject: 0826_1300_539136F ✓ 132 trials\n",
      "\n",
      "Processing subject: 0826_1300_9M4VCHG ✓ 121 trials\n",
      "\n",
      "Processing subject: 0826_1300_U9TEJGM ✓ 78 trials\n",
      "\n",
      "Processing subject: 0827_1000_539136F ✓ 128 trials\n",
      "\n",
      "Processing subject: 0827_1000_9M4VCHG ✓ 131 trials\n",
      "\n",
      "Processing subject: 0827_1000_U9TEJGM ✓ 122 trials\n",
      "\n",
      "Processing subject: 0828_1300_9M4VCHG ✓ 113 trials\n",
      "\n",
      "Processing subject: 0828_1300_U9TEJGM ✓ 74 trials\n",
      "\n",
      "Processing subject: 0830_1300_539136F ✓ 126 trials\n",
      "\n",
      "Processing subject: 0830_1300_9M4VCHG ✓ 131 trials\n",
      "\n",
      "Processing subject: 0830_1300_U9TEJGM ✓ 129 trials\n",
      "\n",
      "Processing subject: 0831_1000_539136F ✓ 131 trials\n",
      "\n",
      "Processing subject: 0831_1000_9M4VCHG ✓ 79 trials\n",
      "\n",
      "Processing subject: 0831_1000_U9TEJGM ✓ 132 trials\n",
      "\n",
      "Processing subject: 0831_1300_539136F ✓ 130 trials\n",
      "\n",
      "Processing subject: 0831_1300_9M4VCHG ✓ 119 trials\n",
      "\n",
      "Processing subject: 0831_1300_U9TEJGM ✓ 129 trials\n",
      "\n",
      "Processing subject: 0901_1000_539136F ✓ 125 trials\n",
      "\n",
      "Processing subject: 0901_1000_9M4VCHG ✓ 124 trials\n",
      "\n",
      "Processing subject: 0901_1000_U9TEJGM ✓ 132 trials\n",
      "\n",
      "Processing subject: 0901_1300_539136F ✓ 116 trials\n",
      "\n",
      "Processing subject: 0901_1300_9M4VCHG ✓ 124 trials\n",
      "\n",
      "Processing subject: 0901_1300_U9TEJGM ✓ 128 trials\n",
      "\n",
      "Processing subject: 0915_1000_539136F ✓ 119 trials\n",
      "\n",
      "Processing subject: 0915_1000_9M4VCHG ✓ 131 trials\n",
      "\n",
      "Processing subject: 0915_1000_U9TEJGM ✓ 82 trials\n",
      "\n",
      "Processing subject: 0917_1030_539136F ✓ 131 trials\n",
      "\n",
      "Processing subject: 0917_1030_9M4VCHG ✓ 129 trials\n",
      "\n",
      "Processing subject: 0917_1030_U9TEJGM ✓ 129 trials\n",
      "\n",
      "Processing subject: 0920_1600_539136F ✓ 128 trials\n",
      "\n",
      "Processing subject: 0920_1600_9M4VCHG ✓ 128 trials\n",
      "\n",
      "Processing subject: 0920_1600_U9TEJGM ✓ 123 trials\n",
      "\n",
      "Processing subject: 0922_1000_539136F ✓ 123 trials\n",
      "\n",
      "Processing subject: 0922_1000_9M4VCHG ✓ 129 trials\n",
      "\n",
      "Processing subject: 0922_1000_U9TEJGM ✓ 110 trials\n",
      "\n",
      "Processing subject: 0923_1000_539136F ✓ 131 trials\n",
      "\n",
      "Processing subject: 0923_1000_9M4VCHG ✓ 131 trials\n",
      "\n",
      "Processing subject: 0923_1000_U9TEJGM ✓ 128 trials\n",
      "\n",
      "Processing subject: 0923_1600_539136F ✓ 121 trials\n",
      "\n",
      "Processing subject: 0923_1600_9M4VCHG ✓ 130 trials\n",
      "\n",
      "Processing subject: 0923_1600_U9TEJGM ✓ 128 trials\n",
      "\n",
      "Processing subject: 0924_1000_539136F ✓ 128 trials\n",
      "\n",
      "Processing subject: 0924_1000_9M4VCHG ✓ 130 trials\n",
      "\n",
      "Processing subject: 0924_1000_U9TEJGM ✓ 120 trials\n",
      "\n",
      "Processing subject: 0924_1600_539136F ✓ 59 trials\n",
      "\n",
      "Processing subject: 0924_1600_9M4VCHG ✓ 130 trials\n",
      "\n",
      "Processing subject: 0924_1600_U9TEJGM ✓ 123 trials\n",
      "\n",
      "Processing subject: 0927_0930_539136F ✓ 131 trials\n",
      "\n",
      "Processing subject: 0927_0930_U9TEJGM ✓ 99 trials\n",
      "\n",
      "Processing subject: 0928_1600_539136F ✓ 110 trials\n",
      "\n",
      "Processing subject: 0928_1600_9M4VCHG ✓ 117 trials\n",
      "\n",
      "Processing subject: 0928_1600_U9TEJGM ✓ 93 trials\n",
      "\n",
      "Processing subject: 0930_1700_539136F ✓ 95 trials\n",
      "\n",
      "Processing subject: 0930_1700_9M4VCHG ✓ 112 trials\n",
      "\n",
      "Processing subject: 0930_1700_U9TEJGM ✓ 123 trials\n",
      "\n",
      "Processing subject: 1005_1600_539136F ✓ 121 trials\n",
      "\n",
      "Processing subject: 1005_1600_9M4VCHG ✓ 123 trials\n",
      "\n",
      "Processing subject: 1005_1600_U9TEJGM ✓ 51 trials\n",
      "\n",
      "================================================================================\n",
      "Total trials extracted: 11467\n"
     ]
    }
   ],
   "source": [
    "# Extract all features\n",
    "all_physiology_features = []\n",
    "all_behavior_features = []\n",
    "all_outcomes = []\n",
    "all_subject_ids = []\n",
    "all_trial_ids = []\n",
    "\n",
    "total_trials = 0\n",
    "\n",
    "for preprocessed_file in preprocessing_files:\n",
    "    with open(preprocessed_file, 'r') as f:\n",
    "        preprocessed = json.load(f)\n",
    "    \n",
    "    subject_id = preprocessed['subject_id']\n",
    "    print(f\"\\nProcessing subject: {subject_id}\", end=\" \")\n",
    "    \n",
    "    matches = list(raw_dir.glob(f\"*{subject_id.split('_')[-1]}.json\"))\n",
    "    pattern = subject_id.replace(\"_\", \".*\")\n",
    "    match = next((f for f in matches if re.search(pattern, f.name)), None)\n",
    "    if not match:\n",
    "        print(\"❌ No matching JSON file\")\n",
    "        continue\n",
    "    \n",
    "    with open(match, 'r') as f:\n",
    "        raw_data = json.load(f)\n",
    "    \n",
    "    subject_trial_count = 0\n",
    "    \n",
    "    for trial_id, trial_data in preprocessed['trial_data'].items():\n",
    "        method_data = trial_data['methods'][baseline_method]\n",
    "        \n",
    "        if method_data['success'] != True:\n",
    "            continue\n",
    "        \n",
    "        raw_trial = raw_data['trials'][int(trial_id)-1]\n",
    "        \n",
    "        if not raw_trial['gamble details']['submitted']:\n",
    "            continue\n",
    "        \n",
    "        # Extract pupil data\n",
    "        time_aligned = np.array(trial_data['time_relative_to_submit'])\n",
    "        pupil_avg = np.array(method_data['pupil_avg_baselined'])\n",
    "        pupil_L = np.array(method_data['pupil_L_baselined'])\n",
    "        pupil_R = np.array(method_data['pupil_R_baselined'])\n",
    "\n",
    "        valid_mask = ~np.isnan(pupil_avg)\n",
    "        pupil_avg_clean = pupil_avg[valid_mask]\n",
    "        pupil_L_clean = pupil_L[valid_mask]\n",
    "        pupil_R_clean = pupil_R[valid_mask]\n",
    "        time_clean = time_aligned[valid_mask]\n",
    "\n",
    "        if len(pupil_avg_clean) < 20:\n",
    "            continue\n",
    "\n",
    "        # Filter to time window (PRE or POST)\n",
    "        if TIMEFRAME == 'PRE':\n",
    "            time_mask = (time_clean >= TIME_WINDOW[0]) & (time_clean < TIME_WINDOW[1])\n",
    "        else:  # POST\n",
    "            time_mask = (time_clean > TIME_WINDOW[0]) & (time_clean <= TIME_WINDOW[1])\n",
    "        \n",
    "        pupil = pupil_avg_clean[time_mask]\n",
    "        pupil_L_filtered = pupil_L_clean[time_mask]\n",
    "        pupil_R_filtered = pupil_R_clean[time_mask]\n",
    "        time_filtered = time_clean[time_mask]\n",
    "\n",
    "        if len(pupil) < 5:\n",
    "            continue\n",
    "\n",
    "        # Calculate derivatives\n",
    "        pupil_velocity = np.diff(pupil) if len(pupil) > 1 else np.array([0])\n",
    "        pupil_acceleration = np.diff(pupil_velocity) if len(pupil_velocity) > 1 else np.array([0])\n",
    "        dilation_mask = pupil_velocity > 0 if len(pupil_velocity) > 0 else np.array([False])\n",
    "\n",
    "        # PHYSIOLOGY FEATURES\n",
    "        physiology_features = {\n",
    "            f'pupil_mean{SUFFIX}': np.mean(pupil),\n",
    "            f'pupil_std{SUFFIX}': np.std(pupil),\n",
    "            f'pupil_slope{SUFFIX}': np.polyfit(time_filtered, pupil, 1)[0] if len(time_filtered) > 1 else 0,\n",
    "            f'time_to_peak{SUFFIX}': time_filtered[np.argmax(pupil)] - time_filtered[0] if len(time_filtered) > 0 else 0,\n",
    "            f'pupil_cv{SUFFIX}': np.std(pupil) / np.abs(np.mean(pupil)) if (len(pupil) > 0 and np.mean(pupil) != 0) else 0,\n",
    "            f'pupil_velocity_mean{SUFFIX}': np.mean(np.abs(pupil_velocity)) if len(pupil_velocity) > 0 else 0,\n",
    "            f'pupil_max_dilation_rate{SUFFIX}': np.max(pupil_velocity) if len(pupil_velocity) > 0 else 0,\n",
    "            f'pupil_max_constriction_rate{SUFFIX}': np.abs(np.min(pupil_velocity)) if len(pupil_velocity) > 0 else 0,\n",
    "            f'pupil_acceleration_std{SUFFIX}': np.std(pupil_acceleration) if len(pupil_acceleration) > 1 else 0,\n",
    "            f'pct_time_dilating{SUFFIX}': np.mean(dilation_mask) if len(dilation_mask) > 0 else 0,\n",
    "            f'num_dilation_peaks{SUFFIX}': np.sum(np.diff(np.sign(pupil_velocity)) > 0) if len(pupil_velocity) > 1 else 0,\n",
    "            f'eye_asymmetry{SUFFIX}': np.nanmean(np.abs(pupil_L_filtered - pupil_R_filtered)) if len(pupil_L_filtered) > 0 else 0,\n",
    "            f'eye_asymmetry_std{SUFFIX}': np.nanstd(pupil_L_filtered - pupil_R_filtered) if len(pupil_L_filtered) > 1 else 0,\n",
    "        }\n",
    "        \n",
    "        # BEHAVIOR FEATURES\n",
    "        gamble_params = raw_trial['gamble details']['gamble parameters']\n",
    "        lct = raw_trial['lct']\n",
    "        \n",
    "        show_screen_time = None\n",
    "        submit_time = None\n",
    "        click_time = None\n",
    "        \n",
    "        for event in lct:\n",
    "            if 'show screen' in event['event']:\n",
    "                show_screen_time = event['time']\n",
    "            elif 'gamble clicked' in event['event']:\n",
    "                click_time = event['time']\n",
    "            elif 'submit' in event['event']:\n",
    "                submit_time = event['time']\n",
    "        \n",
    "        if show_screen_time is None or submit_time is None:\n",
    "            continue\n",
    "        \n",
    "        reaction_time = (click_time - show_screen_time) if click_time else np.nan\n",
    "        decision_time = (submit_time - show_screen_time)\n",
    "\n",
    "        invest_ev = (gamble_params['invest amount 1'] * gamble_params['invest probability 1'] + \n",
    "                    gamble_params['invest amount 2'] * gamble_params['invest probability 2'])\n",
    "        keep_ev = gamble_params['keep amount']\n",
    "        ev_difference = invest_ev - keep_ev\n",
    "\n",
    "        invest_variance = ((gamble_params['invest amount 1'] - invest_ev)**2 * gamble_params['invest probability 1'] +\n",
    "                        (gamble_params['invest amount 2'] - invest_ev)**2 * gamble_params['invest probability 2'])\n",
    "        \n",
    "        final_choice = raw_trial['gamble details']['choices'][-1]['choice'] if len(raw_trial['gamble details']['choices']) > 0 else None\n",
    "        chose_invest = 1 if final_choice == 'INVEST' else 0\n",
    "        \n",
    "        behavior_features = {\n",
    "            'reaction_time': reaction_time if not np.isnan(reaction_time) else decision_time,\n",
    "            'decision_time': decision_time,\n",
    "            'ev_difference': ev_difference,\n",
    "            'invest_variance': invest_variance,\n",
    "            'ambiguity': gamble_params['ambiguity'],\n",
    "            'condition_social': 1 if gamble_params['condition'] == 'social' else 0,\n",
    "            'risk_premium': ev_difference / np.sqrt(invest_variance) if invest_variance > 0 else 0,\n",
    "        }\n",
    "                \n",
    "        outcome = chose_invest\n",
    "        \n",
    "        all_physiology_features.append(physiology_features)\n",
    "        all_behavior_features.append(behavior_features)\n",
    "        all_outcomes.append(outcome)\n",
    "        all_subject_ids.append(subject_id)\n",
    "        all_trial_ids.append(f\"{trial_id}_{subject_id}\")\n",
    "        \n",
    "        subject_trial_count += 1\n",
    "    \n",
    "    print(f\"✓ {subject_trial_count} trials\")\n",
    "    total_trials += subject_trial_count\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Total trials extracted: {total_trials}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Gaze Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gaze_features_from_trial(eye_data, submit_time=None, time_window=(-2.0, 0.0)):\n",
    "    \"\"\"\n",
    "    Extract gaze features from raw eye tracking data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    eye_data : list\n",
    "        Raw eye tracking samples from trial\n",
    "    submit_time : float, optional\n",
    "        Timestamp of submit button press for time-aligning\n",
    "    time_window : tuple, optional\n",
    "        (start, end) in seconds relative to submit_time\n",
    "        Default: (-2.0, 0.0) for PRE-decision period\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict or None\n",
    "        Dictionary of gaze features, or None if insufficient data\n",
    "    \"\"\"\n",
    "    if not eye_data or len(eye_data) == 0:\n",
    "        return None\n",
    "    \n",
    "    timestamps = np.array([s['time'] for s in eye_data])\n",
    "    \n",
    "    # FILTER TO TIME WINDOW if submit_time provided\n",
    "    if submit_time is not None:\n",
    "        time_relative = timestamps - submit_time\n",
    "        time_mask = (time_relative >= time_window[0]) & (time_relative < time_window[1])\n",
    "        \n",
    "        # Filter all data to time window\n",
    "        indices = np.where(time_mask)[0]\n",
    "        if len(indices) < 5:  # Need minimum samples\n",
    "            return None\n",
    "        \n",
    "        # Apply filter\n",
    "        eye_data = [eye_data[i] for i in indices]\n",
    "        timestamps = timestamps[indices]\n",
    "    \n",
    "    # Extract gaze data from filtered eye_data\n",
    "    gaze_x_L = np.array([s.get('gazeL_X', np.nan) for s in eye_data])\n",
    "    gaze_y_L = np.array([s.get('gazeL_Y', np.nan) for s in eye_data])\n",
    "    gaze_x_R = np.array([s.get('gazeR_X', np.nan) for s in eye_data])\n",
    "    gaze_y_R = np.array([s.get('gazeR_Y', np.nan) for s in eye_data])\n",
    "    \n",
    "    gaze_x = np.nanmean([gaze_x_L, gaze_x_R], axis=0)\n",
    "    gaze_y = np.nanmean([gaze_y_L, gaze_y_R], axis=0)\n",
    "    \n",
    "    screen_x_L = np.array([s.get('pupilLSensorPosL_X', np.nan) for s in eye_data])\n",
    "    screen_y_L = np.array([s.get('pupilLSensorPosL_Y', np.nan) for s in eye_data])\n",
    "    screen_x_R = np.array([s.get('pupilLSensorPosR_X', np.nan) for s in eye_data])\n",
    "    screen_y_R = np.array([s.get('pupilLSensorPosR_Y', np.nan) for s in eye_data])\n",
    "    \n",
    "    screen_x = np.nanmean([screen_x_L, screen_x_R], axis=0)\n",
    "    screen_y = np.nanmean([screen_y_L, screen_y_R], axis=0)\n",
    "    \n",
    "    valid_L = np.array([s.get('validL', 0) for s in eye_data])\n",
    "    valid_R = np.array([s.get('validR', 0) for s in eye_data])\n",
    "    \n",
    "    features = {}\n",
    "    features['gaze_valid_pct'] = np.mean((valid_L > 0) & (valid_R > 0))\n",
    "    \n",
    "    valid_mask = (valid_L > 0) & (valid_R > 0)\n",
    "    if valid_mask.sum() < 5:\n",
    "        return None\n",
    "    \n",
    "    gaze_x_valid = gaze_x[valid_mask]\n",
    "    gaze_y_valid = gaze_y[valid_mask]\n",
    "    screen_x_valid = screen_x[valid_mask]\n",
    "    screen_y_valid = screen_y[valid_mask]\n",
    "    timestamps_valid = timestamps[valid_mask]\n",
    "    \n",
    "    features['gaze_x_mean'] = np.nanmean(gaze_x_valid)\n",
    "    features['gaze_x_std'] = np.nanstd(gaze_x_valid)\n",
    "    features['gaze_y_mean'] = np.nanmean(gaze_y_valid)\n",
    "    features['gaze_y_std'] = np.nanstd(gaze_y_valid)\n",
    "    features['screen_x_mean'] = np.nanmean(screen_x_valid)\n",
    "    features['screen_x_std'] = np.nanstd(screen_x_valid)\n",
    "    features['screen_y_mean'] = np.nanmean(screen_y_valid)\n",
    "    features['screen_y_std'] = np.nanstd(screen_y_valid)\n",
    "    \n",
    "    dt = np.diff(timestamps_valid)\n",
    "    dt[dt == 0] = 1e-6\n",
    "    dx = np.diff(screen_x_valid)\n",
    "    dy = np.diff(screen_y_valid)\n",
    "    \n",
    "    velocity = np.sqrt(dx**2 + dy**2) / dt\n",
    "    features['gaze_velocity_mean'] = np.nanmean(velocity)\n",
    "    features['gaze_velocity_std'] = np.nanstd(velocity)\n",
    "    features['gaze_velocity_max'] = np.nanmax(velocity)\n",
    "    \n",
    "    acceleration = np.diff(velocity) / dt[:-1]\n",
    "    features['gaze_acceleration_mean'] = np.nanmean(np.abs(acceleration))\n",
    "    features['gaze_acceleration_std'] = np.nanstd(acceleration)\n",
    "    \n",
    "    fixation_mask = velocity < 30\n",
    "    saccade_mask = velocity > 100\n",
    "    features['fixation_ratio'] = np.mean(fixation_mask)\n",
    "    features['saccade_ratio'] = np.mean(saccade_mask)\n",
    "    features['saccade_count'] = np.sum(np.diff(saccade_mask.astype(int)) == 1)\n",
    "    \n",
    "    features['gaze_dispersion_x'] = np.nanmax(screen_x_valid) - np.nanmin(screen_x_valid)\n",
    "    features['gaze_dispersion_y'] = np.nanmax(screen_y_valid) - np.nanmin(screen_y_valid)\n",
    "    features['gaze_path_length'] = np.sum(np.sqrt(dx**2 + dy**2))\n",
    "    \n",
    "    return features\n",
    "\n",
    "def map_subject_filename(json_filename):\n",
    "    match = re.search(r'(\\d{4})_(\\d{4})_LCT_DESKTOP-([A-Z0-9]+)', json_filename)\n",
    "    if match:\n",
    "        date1, date2, desktop_id = match.groups()\n",
    "        return f\"{date1}_{date2}_{desktop_id}\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting gaze features from raw PKL data for POST-decision period...\n",
      "  Time window: 0.0 to 2.0 seconds\n",
      "  Using preprocessed files to get trial info (avoiding JSON files)\n",
      "  [1/3] Loading PKL files...\n",
      "    Loading PKL 0...\n",
      "    Loading PKL 20...\n",
      "    Loading PKL 40...\n",
      "    Loading PKL 60...\n",
      "    Loading PKL 80...\n",
      "    Loading PKL 100...\n",
      "  Loaded 108 PKL files\n",
      "  [2/3] Building valid trial lookup...\n",
      "  Found 11467 valid trials from preprocessing\n",
      "  [3/3] Extracting gaze features...\n",
      "    Processed 1000 trials, extracted 962 gaze features...\n",
      "    Processed 2000 trials, extracted 1941 gaze features...\n",
      "    Processed 3000 trials, extracted 2908 gaze features...\n",
      "    Processed 4000 trials, extracted 3859 gaze features...\n",
      "    Processed 5000 trials, extracted 4822 gaze features...\n",
      "    Processed 6000 trials, extracted 5792 gaze features...\n",
      "    Processed 7000 trials, extracted 6771 gaze features...\n",
      "    Processed 8000 trials, extracted 7762 gaze features...\n",
      "    Processed 9000 trials, extracted 8739 gaze features...\n",
      "    Processed 10000 trials, extracted 9730 gaze features...\n",
      "    Processed 11000 trials, extracted 10689 gaze features...\n",
      "\n",
      "✓ Extracted gaze features from 11467 trials\n",
      "  Total trials processed: 11794\n",
      "  Skipped: 327 trials\n"
     ]
    }
   ],
   "source": [
    "# Extract gaze features\n",
    "# For PRE-decision: use raw JSON trial['eye'] data (PRE-decision window only)\n",
    "# For POST-decision: use preprocessed PKL data (has POST-decision gaze data!)\n",
    "\n",
    "if TIMEFRAME == 'PRE':\n",
    "    print(f\"Extracting gaze features from raw JSON with {TIMEFRAME}-decision time window filtering...\")\n",
    "\n",
    "    raw_json_files = sorted(raw_dir.glob('*.json'))\n",
    "    all_gaze_data = []\n",
    "    skipped_no_submit = 0\n",
    "\n",
    "    for file_path in raw_json_files:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        subject_id = map_subject_filename(file_path.name)\n",
    "        if not subject_id:\n",
    "            continue\n",
    "\n",
    "        trials = data.get('trials', [])\n",
    "\n",
    "        for trial_idx, trial in enumerate(trials):\n",
    "            eye_data = trial.get('eye', [])\n",
    "            if not eye_data:\n",
    "                continue\n",
    "\n",
    "            # Get submit time from LCT\n",
    "            lct = trial.get('lct', [])\n",
    "            submit_time = None\n",
    "            for event in lct:\n",
    "                if 'submit' in event['event']:\n",
    "                    submit_time = event['time']\n",
    "                    break\n",
    "\n",
    "            if submit_time is None:\n",
    "                skipped_no_submit += 1\n",
    "                continue\n",
    "\n",
    "            gamble_details = trial.get('gamble details', {})\n",
    "            trial_id = str(gamble_details.get('trial', trial_idx))\n",
    "\n",
    "            # Extract gaze features with time window filtering\n",
    "            gaze_features = extract_gaze_features_from_trial(\n",
    "                eye_data,\n",
    "                submit_time=submit_time,\n",
    "                time_window=TIME_WINDOW\n",
    "            )\n",
    "            if gaze_features is None:\n",
    "                continue\n",
    "\n",
    "            gaze_features['subject_id'] = subject_id\n",
    "            gaze_features['trial_id'] = f\"{trial_id}_{subject_id}\"\n",
    "            all_gaze_data.append(gaze_features)\n",
    "\n",
    "    print(f\"Extracted gaze features from {len(all_gaze_data)} trials\")\n",
    "    print(f\"Skipped {skipped_no_submit} trials without submit time\")\n",
    "    print(f\"✓ Gaze features use same time window as pupil features ({TIME_WINDOW[0]} to {TIME_WINDOW[1]}s)\")\n",
    "\n",
    "elif TIMEFRAME == 'POST':\n",
    "    print(f\"Extracting gaze features from raw PKL data for {TIMEFRAME}-decision period...\")\n",
    "    print(f\"  Time window: {TIME_WINDOW[0]} to {TIME_WINDOW[1]} seconds\")\n",
    "    print(f\"  Using preprocessed files to get trial info (avoiding JSON files)\")\n",
    "    \n",
    "    raw_pkl_dir = Path('../../data/raw/eye')\n",
    "    \n",
    "    # Step 1: Pre-load all PKL files\n",
    "    print(f\"  [1/3] Loading PKL files...\")\n",
    "    pkl_cache = {}\n",
    "    for i, pkl_file in enumerate(raw_pkl_dir.glob('*.pkl')):\n",
    "        if i % 20 == 0:\n",
    "            print(f\"    Loading PKL {i}...\")\n",
    "        subject_id_from_pkl = pkl_file.stem\n",
    "        try:\n",
    "            with open(pkl_file, 'rb') as f:\n",
    "                eye_pkl = pickle.load(f)\n",
    "            if not isinstance(eye_pkl, pd.DataFrame):\n",
    "                eye_pkl = pd.DataFrame(eye_pkl)\n",
    "            eye_pkl = eye_pkl.sort_values('time').reset_index(drop=True)\n",
    "            pkl_cache[subject_id_from_pkl] = eye_pkl\n",
    "        except Exception as e:\n",
    "            print(f\"    ⚠ Error loading {pkl_file.name}: {e}\")\n",
    "    print(f\"  Loaded {len(pkl_cache)} PKL files\")\n",
    "    \n",
    "    # Step 2: Build lookup of valid trials from all_trial_ids\n",
    "    # These are trials that already passed all validation checks\n",
    "    print(f\"  [2/3] Building valid trial lookup...\")\n",
    "    valid_trials = set(all_trial_ids)\n",
    "    print(f\"  Found {len(valid_trials)} valid trials from preprocessing\")\n",
    "    \n",
    "    # Step 3: Extract gaze features\n",
    "    print(f\"  [3/3] Extracting gaze features...\")\n",
    "    all_gaze_data = []\n",
    "    skipped_count = 0\n",
    "    trials_processed = 0\n",
    "    \n",
    "    for preprocessed_file in preprocessing_files:\n",
    "        with open(preprocessed_file, 'r') as f:\n",
    "            preprocessed = json.load(f)\n",
    "        \n",
    "        subject_id = preprocessed['subject_id']\n",
    "        \n",
    "        # Check if PKL exists\n",
    "        if subject_id not in pkl_cache:\n",
    "            continue\n",
    "        \n",
    "        eye_pkl = pkl_cache[subject_id]\n",
    "        \n",
    "        # Process each trial\n",
    "        for trial_id, trial_data in preprocessed['trial_data'].items():\n",
    "            trials_processed += 1\n",
    "            if trials_processed % 1000 == 0:\n",
    "                print(f\"    Processed {trials_processed} trials, extracted {len(all_gaze_data)} gaze features...\")\n",
    "            \n",
    "            # Skip if trial not in valid set (already filtered in physiology extraction)\n",
    "            trial_key = f\"{trial_id}_{subject_id}\"\n",
    "            if trial_key not in valid_trials:\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Get submit time from preprocessed data\n",
    "            # time_relative_to_submit has times relative to submit, so we can calculate submit_time\n",
    "            time_relative = np.array(trial_data['time_relative_to_submit'])\n",
    "            time_absolute = np.array(trial_data['time'])\n",
    "            \n",
    "            if len(time_relative) == 0 or len(time_absolute) == 0:\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Calculate submit time: absolute_time = submit_time + relative_time\n",
    "            # So: submit_time = absolute_time - relative_time\n",
    "            submit_time = time_absolute[0] - time_relative[0]\n",
    "            \n",
    "            # Extract POST-decision window from PKL using time-based filtering\n",
    "            post_window_start = submit_time\n",
    "            post_window_end = submit_time + 2000  # 2 seconds\n",
    "            \n",
    "            # Simple time filter (very fast)\n",
    "            post_decision_data = eye_pkl[\n",
    "                (eye_pkl['time'] > post_window_start) & \n",
    "                (eye_pkl['time'] <= post_window_end)\n",
    "            ]\n",
    "            \n",
    "            if len(post_decision_data) < 5:\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Convert to list of dicts\n",
    "            eye_data_list = post_decision_data.to_dict('records')\n",
    "            \n",
    "            # Extract gaze features\n",
    "            gaze_features = extract_gaze_features_from_trial(\n",
    "                eye_data_list,\n",
    "                submit_time=None,\n",
    "                time_window=None\n",
    "            )\n",
    "            \n",
    "            if gaze_features is None:\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            \n",
    "            gaze_features['subject_id'] = subject_id\n",
    "            gaze_features['trial_id'] = f\"{trial_id}_{subject_id}\"\n",
    "            all_gaze_data.append(gaze_features)\n",
    "    \n",
    "    print(f\"\\n✓ Extracted gaze features from {len(all_gaze_data)} trials\")\n",
    "    print(f\"  Total trials processed: {trials_processed}\")\n",
    "    print(f\"  Skipped: {skipped_count} trials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create DataFrames and Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL MERGED DATASET (POST-DECISION):\n",
      "================================================================================\n",
      "Total trials: 11467\n",
      "Subjects: 97\n",
      "\n",
      "Feature counts:\n",
      "  Physiology (POST): 13 features\n",
      "  Behavior: 7 features (reaction_time, decision_time, ev_difference, invest_variance, ambiguity, condition_social, risk_premium)\n",
      "  Gaze: 17 features\n",
      "\n",
      "Outcome distribution:\n",
      "outcome\n",
      "1    7550\n",
      "0    3917\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create dataframes\n",
    "physio_df = pd.DataFrame(all_physiology_features)\n",
    "physio_df.insert(0, 'subject_id', all_subject_ids)\n",
    "physio_df.insert(1, 'trial_id', all_trial_ids)\n",
    "physio_df['outcome'] = all_outcomes\n",
    "\n",
    "behavior_df = pd.DataFrame(all_behavior_features)\n",
    "behavior_df.insert(0, 'subject_id', all_subject_ids)\n",
    "behavior_df.insert(1, 'trial_id', all_trial_ids)\n",
    "behavior_df['outcome'] = all_outcomes\n",
    "\n",
    "gaze_df = pd.DataFrame(all_gaze_data)\n",
    "\n",
    "# Merge all modalities\n",
    "merged_df = physio_df.merge(\n",
    "    behavior_df.drop(columns=['outcome']),\n",
    "    on=['subject_id', 'trial_id'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Only merge gaze data if available (PRE-decision only)\n",
    "if len(gaze_df) > 0:\n",
    "    merged_df = merged_df.merge(\n",
    "        gaze_df,\n",
    "        on=['subject_id', 'trial_id'],\n",
    "        how='inner'\n",
    "    )\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"FINAL MERGED DATASET ({TIMEFRAME}-DECISION):\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total trials: {len(merged_df)}\")\n",
    "print(f\"Subjects: {merged_df['subject_id'].nunique()}\")\n",
    "print(f\"\\nFeature counts:\")\n",
    "print(f\"  Physiology ({TIMEFRAME}): {len([c for c in merged_df.columns if c.endswith(SUFFIX)])} features\")\n",
    "print(f\"  Behavior: 7 features (reaction_time, decision_time, ev_difference, invest_variance, ambiguity, condition_social, risk_premium)\")\n",
    "if len(gaze_df) > 0:\n",
    "    print(f\"  Gaze: {len([c for c in merged_df.columns if c.startswith('gaze_') or c.startswith('screen_')])} features\")\n",
    "else:\n",
    "    print(f\"  Gaze: 0 features (not available for {TIMEFRAME}-decision)\")\n",
    "print(f\"\\nOutcome distribution:\")\n",
    "print(merged_df['outcome'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save to Pickle File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "✓ POST-decision features saved to: ../../data/results/features_POST/extracted_features_POST.pkl\n",
      "  File size: 8.01 MB\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Save to pickle\n",
    "output_dir = Path(f'../../data/results/features_{TIMEFRAME}')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_file = output_dir / f'extracted_features_{TIMEFRAME}.pkl'\n",
    "\n",
    "# Define feature column names\n",
    "physio_cols = [c for c in merged_df.columns if c.endswith(SUFFIX)]\n",
    "behavior_cols = ['reaction_time', 'decision_time', 'ev_difference',\n",
    "                 'invest_variance', 'ambiguity', 'condition_social', 'risk_premium']\n",
    "if len(gaze_df) > 0:\n",
    "    gaze_cols = [c for c in merged_df.columns\n",
    "                 if c.startswith('gaze_') or c.startswith('screen_') or\n",
    "                 c in ['fixation_ratio', 'saccade_ratio', 'saccade_count', 'gaze_valid_pct',\n",
    "                       'gaze_dispersion_x', 'gaze_dispersion_y', 'gaze_path_length']]\n",
    "else:\n",
    "    gaze_cols = []\n",
    "\n",
    "# Save with metadata\n",
    "feature_data = {\n",
    "    'merged_df': merged_df,\n",
    "    'physio_df': physio_df,\n",
    "    'behavior_df': behavior_df,\n",
    "    'gaze_df': gaze_df,\n",
    "    'physio_cols': physio_cols,\n",
    "    'behavior_cols': behavior_cols,\n",
    "    'gaze_cols': gaze_cols,\n",
    "    'metadata': {\n",
    "        'extraction_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'n_trials': len(merged_df),\n",
    "        'n_subjects': merged_df['subject_id'].nunique(),\n",
    "        'baseline_method': baseline_method,\n",
    "        'preprocessing_files': len(preprocessing_files),\n",
    "        'time_window': f'{TIMEFRAME}-decision ({TIME_WINDOW[0]} to {TIME_WINDOW[1]} seconds)',\n",
    "        'has_gaze_features': len(gaze_df) > 0\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(feature_data, f)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ {TIMEFRAME}-decision features saved to: {output_file}\")\n",
    "print(f\"  File size: {output_file.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "liinc_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
