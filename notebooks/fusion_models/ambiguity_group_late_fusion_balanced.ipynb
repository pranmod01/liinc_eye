{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Late Fusion by Ambiguity Groups - BALANCED\n",
    "\n",
    "This notebook splits trials into ambiguity groups with **STRATIFIED SAMPLING** to ensure each subject contributes equally to each group.\n",
    "\n",
    "**Parameterized**: Set `TIMEFRAME` to run PRE or POST analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "AMBIGUITY GROUP (BALANCED) ANALYSIS: PRE-DECISION PERIOD\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION: Set timeframe for analysis\n",
    "# ============================================================================\n",
    "TIMEFRAME = 'PRE'  # Options: 'PRE', 'POST'\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')  # Add project root to path\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import from src package\n",
    "from src.models.fusion import weighted_late_fusion\n",
    "\n",
    "np.random.seed(42)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"AMBIGUITY GROUP (BALANCED) ANALYSIS: {TIMEFRAME}-DECISION PERIOD\")\n",
    "print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Pre-Extracted Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 12511 trials from 97 subjects\n",
      "  Features extracted on: 2026-01-01 17:53:05\n",
      "\n",
      "Ambiguity distribution:\n",
      "ambiguity\n",
      "0    4177\n",
      "3    4162\n",
      "6    4172\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load pre-extracted features\n",
    "with open(f'../../data/results/features_{TIMEFRAME}/extracted_features_{TIMEFRAME}.pkl', 'rb') as f:\n",
    "    feature_data = pickle.load(f)\n",
    "\n",
    "merged_df = feature_data['merged_df']\n",
    "physio_cols = feature_data['physio_cols']\n",
    "behavior_cols = feature_data['behavior_cols']\n",
    "gaze_cols = feature_data['gaze_cols']\n",
    "\n",
    "print(f\"✓ Loaded {len(merged_df)} trials from {merged_df['subject_id'].nunique()} subjects\")\n",
    "print(f\"  Features extracted on: {feature_data['metadata']['extraction_date']}\")\n",
    "print(f\"\\nAmbiguity distribution:\")\n",
    "print(merged_df['ambiguity'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Ambiguity Groups with Stratified Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original ambiguity group distribution:\n",
      "ambiguity_group\n",
      "Low       4177\n",
      "High      4172\n",
      "Medium    4162\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n",
      "STRATIFIED SAMPLING: Ensuring equal subject representation\n",
      "================================================================================\n",
      "\n",
      "Low Ambiguity:\n",
      "  Original: 4177 trials\n",
      "  Min trials per subject: 4\n",
      "  Sampling 4 trials from each of 24 subjects\n",
      "  After balancing: 388 trials (9.3% of original)\n",
      "\n",
      "Medium Ambiguity:\n",
      "  Original: 4162 trials\n",
      "  Min trials per subject: 2\n",
      "  Sampling 2 trials from each of 25 subjects\n",
      "  After balancing: 194 trials (4.7% of original)\n",
      "\n",
      "High Ambiguity:\n",
      "  Original: 4172 trials\n",
      "  Min trials per subject: 3\n",
      "  Sampling 3 trials from each of 25 subjects\n",
      "  After balancing: 291 trials (7.0% of original)\n",
      "\n",
      "================================================================================\n",
      "SUMMARY:\n",
      "  Original total: 12511 trials\n",
      "  Balanced total: 873 trials\n",
      "  Data retained: 7.0%\n",
      "  Data lost: 93.0%\n",
      "\n",
      "Balanced distribution:\n",
      "ambiguity_group\n",
      "Low       388\n",
      "High      291\n",
      "Medium    194\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create ambiguity groups\n",
    "merged_df['ambiguity_group'] = merged_df['ambiguity'].replace({0:'Low', 3:'Medium', 6:'High'})\n",
    "\n",
    "print(\"Original ambiguity group distribution:\")\n",
    "print(merged_df['ambiguity_group'].value_counts())\n",
    "\n",
    "# Perform stratified sampling to balance subjects across groups\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STRATIFIED SAMPLING: Ensuring equal subject representation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "balanced_data = []\n",
    "for group in ['Low', 'Medium', 'High']:\n",
    "    group_data = merged_df[merged_df['ambiguity_group'] == group].copy()\n",
    "    \n",
    "    # Find minimum trials per subject in this group\n",
    "    subject_counts = group_data.groupby('subject_id').size()\n",
    "    min_trials = subject_counts.min()\n",
    "    \n",
    "    print(f\"\\n{group} Ambiguity:\")\n",
    "    print(f\"  Original: {len(group_data)} trials\")\n",
    "    print(f\"  Min trials per subject: {min_trials}\")\n",
    "    print(f\"  Sampling {min_trials} trials from each of {subject_counts.nunique()} subjects\")\n",
    "    \n",
    "    # Sample exactly min_trials from each subject\n",
    "    balanced_group_data = []\n",
    "    for subject in group_data['subject_id'].unique():\n",
    "        subject_data = group_data[group_data['subject_id'] == subject]\n",
    "        sampled = subject_data.sample(n=min_trials, random_state=42)\n",
    "        balanced_group_data.append(sampled)\n",
    "    \n",
    "    balanced_group = pd.concat(balanced_group_data, ignore_index=True)\n",
    "    print(f\"  After balancing: {len(balanced_group)} trials ({len(balanced_group) / len(group_data) * 100:.1f}% of original)\")\n",
    "    balanced_data.append(balanced_group)\n",
    "\n",
    "merged_df_balanced = pd.concat(balanced_data, ignore_index=True)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SUMMARY:\")\n",
    "print(f\"  Original total: {len(merged_df)} trials\")\n",
    "print(f\"  Balanced total: {len(merged_df_balanced)} trials\")\n",
    "print(f\"  Data retained: {len(merged_df_balanced) / len(merged_df) * 100:.1f}%\")\n",
    "print(f\"  Data lost: {(1 - len(merged_df_balanced) / len(merged_df)) * 100:.1f}%\")\n",
    "print(f\"\\nBalanced distribution:\")\n",
    "print(merged_df_balanced['ambiguity_group'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Feature Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physiology (PRE): 13 features\n",
      "Behavior: 6 features\n",
      "Gaze: 20 features\n"
     ]
    }
   ],
   "source": [
    "physio_cols = [c for c in merged_df_balanced.columns if c.endswith('_pre')]\n",
    "behavior_cols = ['reaction_time', 'decision_time', 'ev_difference', \n",
    "                 'invest_variance', 'condition_social', 'risk_premium']\n",
    "gaze_cols = [c for c in merged_df_balanced.columns \n",
    "             if c.startswith('gaze_') or c.startswith('screen_') or \n",
    "             c in ['fixation_ratio', 'saccade_ratio', 'saccade_count', 'gaze_valid_pct',\n",
    "                   'gaze_dispersion_x', 'gaze_dispersion_y', 'gaze_path_length']]\n",
    "\n",
    "print(f\"Physiology ({TIMEFRAME}): {len(physio_cols)} features\")\n",
    "print(f\"Behavior: {len(behavior_cols)} features\")\n",
    "print(f\"Gaze: {len(gaze_cols)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Weighted Late Fusion on BALANCED Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Analyzing Low Ambiguity Group\n",
      "================================================================================\n",
      "\n",
      "Running weighted late fusion for Low group...\n",
      "  N trials: 388\n",
      "  N subjects: 97\n"
     ]
    }
   ],
   "source": [
    "# Run late fusion for each ambiguity group\n",
    "group_results = {}\n",
    "\n",
    "for group in ['Low', 'Medium', 'High']:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Analyzing {group} Ambiguity Group\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Filter data for this group\n",
    "    group_df = merged_df_balanced[merged_df_balanced['ambiguity_group'] == group].copy()\n",
    "    \n",
    "    # Prepare feature matrices\n",
    "    X_physio = group_df[physio_cols].values if len(physio_cols) > 0 else np.zeros((len(group_df), 1))\n",
    "    X_behavior = group_df[behavior_cols].values\n",
    "    X_gaze = group_df[gaze_cols].values if len(gaze_cols) > 0 else np.zeros((len(group_df), 1))\n",
    "    \n",
    "    # Handle missing values\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_physio = imputer.fit_transform(X_physio)\n",
    "    X_behavior = imputer.fit_transform(X_behavior)\n",
    "    X_gaze = imputer.fit_transform(X_gaze)\n",
    "    \n",
    "    y = group_df['outcome'].values\n",
    "    subjects = group_df['subject_id'].values\n",
    "    \n",
    "    # Setup modalities based on available data\n",
    "    if len(gaze_cols) > 0:\n",
    "        X_modalities = [X_physio, X_behavior, X_gaze]\n",
    "        modality_names = ['Physiology', 'Behavior', 'Gaze']\n",
    "    else:\n",
    "        # POST condition: no gaze data\n",
    "        X_modalities = [X_physio, X_behavior]\n",
    "        modality_names = ['Physiology', 'Behavior']\n",
    "    \n",
    "    print(f\"\\nRunning weighted late fusion for {group} group...\")\n",
    "    print(f\"  N trials: {len(y)}\")\n",
    "    print(f\"  N subjects: {len(np.unique(subjects))}\")\n",
    "    \n",
    "    # Run fusion\n",
    "    results = weighted_late_fusion(X_modalities, y, subjects, modality_names)\n",
    "    group_results[group] = results\n",
    "    \n",
    "    print(f\"\\nResults for {group} group:\")\n",
    "    print(f\"  Accuracy: {results['accuracy_mean']:.3f} ± {results['accuracy_sem']:.3f} (SEM)\")\n",
    "    print(f\"  F1-Score: {results['f1_mean']:.3f} ± {results['f1_sem']:.3f} (SEM)\")\n",
    "    print(f\"  Weights: {dict(zip(modality_names, results['weights']))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Results Across Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'Group': group,\n",
    "        'N_Trials': results['n_trials'],\n",
    "        'N_Subjects': results['n_subjects'],\n",
    "        'Accuracy': results['accuracy_mean'],\n",
    "        'Accuracy_SEM': results['accuracy_sem'],\n",
    "        'Accuracy_SD': results['accuracy_std'],\n",
    "        'F1-Score': results['f1_mean'],\n",
    "        'F1_SEM': results['f1_sem'],\n",
    "        'Physiology_Weight': results['weights'][0] if len(results['weights']) > 0 else 0,\n",
    "        'Behavior_Weight': results['weights'][1] if len(results['weights']) > 1 else 0,\n",
    "        'Gaze_Weight': results['weights'][2] if len(results['weights']) > 2 else 0\n",
    "    }\n",
    "    for group, results in group_results.items()\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON ACROSS AMBIGUITY GROUPS (BALANCED DATA with SEM)\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df[['Group', 'N_Trials', 'N_Subjects', 'Accuracy', 'Accuracy_SEM', 'Accuracy_SD']].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NOTE: Using BALANCED/STRATIFIED data\")\n",
    "print(\"- Each subject contributes EQUALLY to each ambiguity group\")\n",
    "print(\"- Error bars use SEM (Standard Error of Mean) from subject-level accuracies\")\n",
    "print(\"- No subject imbalance bias in LOSO CV\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy with SEM error bars\n",
    "ax = axes[0]\n",
    "ax.errorbar(comparison_df['Group'], comparison_df['Accuracy'], \n",
    "            yerr=comparison_df['Accuracy_SEM'], fmt='o-', capsize=5, color='steelblue', markersize=8)\n",
    "ax.set_xlabel('Ambiguity Group')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Accuracy by Ambiguity Group (error bars = SEM)')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.axhline(0.5, color='red', linestyle='--', alpha=0.3)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# F1-Score with SEM error bars\n",
    "ax = axes[1]\n",
    "ax.errorbar(comparison_df['Group'], comparison_df['F1-Score'], \n",
    "            yerr=comparison_df['F1_SEM'], fmt='o-', capsize=5, color='coral', markersize=8)\n",
    "ax.set_xlabel('Ambiguity Group')\n",
    "ax.set_ylabel('F1-Score')\n",
    "ax.set_title('F1-Score by Ambiguity Group (error bars = SEM)')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.axhline(0.5, color='red', linestyle='--', alpha=0.3)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Subject-Level Accuracy Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot subject-level accuracy distributions for each ambiguity group\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for idx, (group, results) in enumerate(group_results.items()):\n",
    "    ax = axes[idx]\n",
    "    subject_accs = results['accuracy_per_subject']\n",
    "    \n",
    "    # Histogram\n",
    "    ax.hist(subject_accs, bins=6, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    ax.axvline(results['accuracy_mean'], color='red', linestyle='--', linewidth=2,\n",
    "               label=f\"Mean: {results['accuracy_mean']:.3f}\")\n",
    "    ax.axvline(results['accuracy_mean'] - results['accuracy_sem'], color='orange',\n",
    "               linestyle=':', linewidth=1.5, label=f\"±SEM\")\n",
    "    ax.axvline(results['accuracy_mean'] + results['accuracy_sem'], color='orange',\n",
    "               linestyle=':', linewidth=1.5)\n",
    "    \n",
    "    ax.set_xlabel('Accuracy')\n",
    "    ax.set_ylabel('Count (subjects)')\n",
    "    ax.set_title(f'{group} Ambiguity (n={results[\"n_subjects\"]} subjects)')\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Subject-Level Accuracy Distribution by Ambiguity Group', fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modality weights by group\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.25\n",
    "\n",
    "ax.bar(x - width, comparison_df['Physiology_Weight'], width, \n",
    "       label=f'Physiology ({TIMEFRAME})', color='steelblue')\n",
    "ax.bar(x, comparison_df['Behavior_Weight'], width, \n",
    "       label='Behavior', color='coral')\n",
    "ax.bar(x + width, comparison_df['Gaze_Weight'], width, \n",
    "       label='Gaze', color='mediumseagreen')\n",
    "\n",
    "ax.set_xlabel('Ambiguity Group')\n",
    "ax.set_ylabel('Weight')\n",
    "ax.set_title('Modality Weights by Ambiguity Group')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison_df['Group'])\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results to CSV\n",
    "\n",
    "Exporting all results for later analysis without re-running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(f'../../data/results/fusion_model_results_{TIMEFRAME}', exist_ok=True)\n",
    "\n",
    "# Save comparison results across groups\n",
    "if 'comparison_df' in locals() and comparison_df is not None:\n",
    "    output_file = f'../../data/results/fusion_model_results_{TIMEFRAME}/ambiguity_group_late_fusion_balanced_{TIMEFRAME}_comparison.csv'\n",
    "    comparison_df.to_csv(output_file, index=False)\n",
    "    print(f\"✓ Saved comparison results to: {output_file}\")\n",
    "    print(f\"  Contains {len(comparison_df)} groups\")\n",
    "\n",
    "# Save detailed group results\n",
    "if 'group_results' in locals() and group_results:\n",
    "    for group_name, results in group_results.items():\n",
    "        # Save modality weights\n",
    "        modality_names = results.get('modality_names', ['Physiology', 'Behavior', 'Gaze'][:len(results['weights'])])\n",
    "        weights_df = pd.DataFrame({\n",
    "            'Modality': modality_names,\n",
    "            'Weight': results['weights']\n",
    "        })\n",
    "        weights_file = f'../../data/results/fusion_model_results_{TIMEFRAME}/ambiguity_group_late_fusion_balanced_{TIMEFRAME}_{group_name}_weights.csv'\n",
    "        weights_df.to_csv(weights_file, index=False)\n",
    "\n",
    "        # Save subject-level accuracies if available\n",
    "        if 'accuracy_per_subject' in results:\n",
    "            subject_df = pd.DataFrame({\n",
    "                'accuracy': results['accuracy_per_subject'],\n",
    "                'f1_score': results.get('f1_per_subject', [])\n",
    "            })\n",
    "            subject_file = f'../../data/results/fusion_model_results_{TIMEFRAME}/ambiguity_group_late_fusion_balanced_{TIMEFRAME}_{group_name}_subject_accuracies.csv'\n",
    "            subject_df.to_csv(subject_file, index=False)\n",
    "\n",
    "    print(f\"✓ Saved detailed results for {len(group_results)} groups\")\n",
    "\n",
    "print(f\"\\nAll results saved to: ../../data/results/fusion_model_results_{TIMEFRAME}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "liinc_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
