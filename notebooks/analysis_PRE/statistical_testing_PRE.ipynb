{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Significance Testing (PRE-Decision)\n",
    "\n",
    "This notebook performs rigorous statistical tests to validate that our model performance is:\n",
    "1. **Significantly better than chance** (permutation test)\n",
    "2. **Reliable** (bootstrap confidence intervals)\n",
    "3. **Significantly different between models** (McNemar's test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PRE-Decision Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trials: 12511\n",
      "Subjects: 97\n",
      "Outcome distribution: {1: 8238, 0: 4273}\n"
     ]
    }
   ],
   "source": [
    "# Load features\n",
    "with open('../../data/results/features_PRE/extracted_features_PRE.pkl', 'rb') as f:\n",
    "    feature_data = pickle.load(f)\n",
    "\n",
    "data = feature_data['merged_df']\n",
    "\n",
    "print(f\"Total trials: {len(data)}\")\n",
    "print(f\"Subjects: {data['subject_id'].nunique()}\")\n",
    "print(f\"Outcome distribution: {data['outcome'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Permutation Test: Better Than Chance?\n",
    "\n",
    "**Null hypothesis:** Model accuracy = 50% (chance)\n",
    "\n",
    "**Method:** Shuffle labels 1000 times, recompute accuracy, compare to observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_test(X, y, subjects, n_permutations=1000, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Permutation test to assess if accuracy is significantly > chance.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Permutation Test: {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Get true accuracy\n",
    "    logo = LeaveOneGroupOut()\n",
    "    true_accs = []\n",
    "    \n",
    "    for train_idx, test_idx in logo.split(X, y, subjects):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        model = RandomForestClassifier(n_estimators=100, max_depth=5, \n",
    "                                      min_samples_split=10, min_samples_leaf=5, \n",
    "                                      random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        true_accs.append(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    true_accuracy = np.mean(true_accs)\n",
    "    print(f\"True Accuracy: {true_accuracy:.4f}\")\n",
    "    \n",
    "    # Permutation null distribution\n",
    "    null_accs = []\n",
    "    \n",
    "    for i in tqdm(range(n_permutations), desc=\"Permutations\"):\n",
    "        # Shuffle labels\n",
    "        y_shuffled = np.random.permutation(y)\n",
    "        \n",
    "        # Recompute accuracy with shuffled labels\n",
    "        perm_accs = []\n",
    "        for train_idx, test_idx in logo.split(X, y_shuffled, subjects):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y_shuffled[train_idx], y_shuffled[test_idx]\n",
    "            \n",
    "            model = RandomForestClassifier(n_estimators=100, max_depth=5,\n",
    "                                          min_samples_split=10, min_samples_leaf=5,\n",
    "                                          random_state=i)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            perm_accs.append(accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        null_accs.append(np.mean(perm_accs))\n",
    "    \n",
    "    # Compute p-value\n",
    "    p_value = np.mean(np.array(null_accs) >= true_accuracy)\n",
    "    \n",
    "    print(f\"\\nNull Distribution Mean: {np.mean(null_accs):.4f}\")\n",
    "    print(f\"Null Distribution Std: {np.std(null_accs):.4f}\")\n",
    "    print(f\"\\np-value: {p_value:.4f}\")\n",
    "    \n",
    "    if p_value < 0.001:\n",
    "        print(f\"✓ {model_name} accuracy is HIGHLY SIGNIFICANT (p < 0.001)\")\n",
    "    elif p_value < 0.01:\n",
    "        print(f\"✓ {model_name} accuracy is VERY SIGNIFICANT (p < 0.01)\")\n",
    "    elif p_value < 0.05:\n",
    "        print(f\"✓ {model_name} accuracy is SIGNIFICANT (p < 0.05)\")\n",
    "    else:\n",
    "        print(f\"✗ {model_name} accuracy is NOT SIGNIFICANT (p = {p_value:.4f})\")\n",
    "    \n",
    "    return {\n",
    "        'true_accuracy': true_accuracy,\n",
    "        'null_mean': np.mean(null_accs),\n",
    "        'null_std': np.std(null_accs),\n",
    "        'p_value': p_value,\n",
    "        'null_distribution': null_accs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Permutation Test: Gaze Model (PRE)\n",
      "======================================================================\n",
      "True Accuracy: 0.6566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Permutations:   0%|          | 3/1000 [06:49<38:31:57, 139.13s/it]"
     ]
    }
   ],
   "source": [
    "# Define feature groups (from extraction notebook)\n",
    "gaze_features = [\n",
    "    'gaze_valid_pct', 'gaze_x_mean', 'gaze_x_std', 'gaze_y_mean', 'gaze_y_std',\n",
    "    'screen_x_mean', 'screen_x_std', 'screen_y_mean', 'screen_y_std',\n",
    "    'gaze_velocity_mean', 'gaze_velocity_std', 'gaze_velocity_max',\n",
    "    'gaze_acceleration_mean', 'gaze_acceleration_std',\n",
    "    'fixation_ratio', 'saccade_ratio', 'saccade_count',\n",
    "    'gaze_dispersion_x', 'gaze_dispersion_y', 'gaze_path_length'\n",
    "]\n",
    "\n",
    "# Filter available features\n",
    "available_gaze = [f for f in gaze_features if f in data.columns]\n",
    "\n",
    "# Prepare data\n",
    "X_gaze = SimpleImputer(strategy='mean').fit_transform(data[available_gaze])\n",
    "y = data['outcome'].values\n",
    "subjects = data['subject_id'].values\n",
    "\n",
    "# Run permutation test on gaze model (best performing)\n",
    "perm_results = permutation_test(X_gaze, y, subjects, \n",
    "                               n_permutations=1000, \n",
    "                               model_name=\"Gaze Model (PRE)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize permutation test\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(perm_results['null_distribution'], bins=50, alpha=0.7, \n",
    "         color='gray', edgecolor='black', label='Null Distribution')\n",
    "plt.axvline(perm_results['true_accuracy'], color='red', linewidth=3, \n",
    "           label=f\"True Accuracy = {perm_results['true_accuracy']:.4f}\")\n",
    "plt.axvline(perm_results['null_mean'], color='blue', linewidth=2, \n",
    "           linestyle='--', label=f\"Null Mean = {perm_results['null_mean']:.4f}\")\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f\"Permutation Test: Gaze Model (PRE)\\np-value = {perm_results['p_value']:.4f}\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bootstrap Confidence Intervals\n",
    "\n",
    "**Goal:** Estimate 95% CI for accuracy and F1-score\n",
    "\n",
    "**Method:** Resample subjects with replacement 1000 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_ci(X, y, subjects, n_bootstrap=1000, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Bootstrap confidence intervals for accuracy and F1-score.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Bootstrap Confidence Intervals: {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    unique_subjects = np.unique(subjects)\n",
    "    n_subjects = len(unique_subjects)\n",
    "    \n",
    "    boot_accs = []\n",
    "    boot_f1s = []\n",
    "    \n",
    "    for i in tqdm(range(n_bootstrap), desc=\"Bootstrap iterations\"):\n",
    "        # Resample subjects with replacement\n",
    "        boot_subjects = np.random.choice(unique_subjects, size=n_subjects, replace=True)\n",
    "        \n",
    "        # Create bootstrap sample\n",
    "        boot_mask = np.isin(subjects, boot_subjects)\n",
    "        X_boot = X[boot_mask]\n",
    "        y_boot = y[boot_mask]\n",
    "        subjects_boot = subjects[boot_mask]\n",
    "        \n",
    "        # LOSO CV on bootstrap sample\n",
    "        logo = LeaveOneGroupOut()\n",
    "        fold_accs = []\n",
    "        fold_f1s = []\n",
    "        \n",
    "        for train_idx, test_idx in logo.split(X_boot, y_boot, subjects_boot):\n",
    "            X_train, X_test = X_boot[train_idx], X_boot[test_idx]\n",
    "            y_train, y_test = y_boot[train_idx], y_boot[test_idx]\n",
    "            \n",
    "            model = RandomForestClassifier(n_estimators=100, max_depth=5,\n",
    "                                          min_samples_split=10, min_samples_leaf=5,\n",
    "                                          random_state=i)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            fold_accs.append(accuracy_score(y_test, y_pred))\n",
    "            fold_f1s.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "        \n",
    "        boot_accs.append(np.mean(fold_accs))\n",
    "        boot_f1s.append(np.mean(fold_f1s))\n",
    "    \n",
    "    # Compute 95% CI\n",
    "    acc_ci = np.percentile(boot_accs, [2.5, 97.5])\n",
    "    f1_ci = np.percentile(boot_f1s, [2.5, 97.5])\n",
    "    \n",
    "    print(f\"\\nAccuracy: {np.mean(boot_accs):.4f} (95% CI: {acc_ci[0]:.4f}-{acc_ci[1]:.4f})\")\n",
    "    print(f\"F1-Score: {np.mean(boot_f1s):.4f} (95% CI: {f1_ci[0]:.4f}-{f1_ci[1]:.4f})\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy_mean': np.mean(boot_accs),\n",
    "        'accuracy_ci': acc_ci,\n",
    "        'f1_mean': np.mean(boot_f1s),\n",
    "        'f1_ci': f1_ci,\n",
    "        'boot_accs': boot_accs,\n",
    "        'boot_f1s': boot_f1s\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run bootstrap for gaze model\n",
    "boot_results = bootstrap_ci(X_gaze, y, subjects, \n",
    "                           n_bootstrap=1000, \n",
    "                           model_name=\"Gaze Model (PRE)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize bootstrap distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy distribution\n",
    "axes[0].hist(boot_results['boot_accs'], bins=30, alpha=0.7, \n",
    "            color='steelblue', edgecolor='black')\n",
    "axes[0].axvline(boot_results['accuracy_mean'], color='red', linewidth=2, \n",
    "               label=f\"Mean = {boot_results['accuracy_mean']:.4f}\")\n",
    "axes[0].axvline(boot_results['accuracy_ci'][0], color='orange', \n",
    "               linewidth=2, linestyle='--', label=f\"95% CI\")\n",
    "axes[0].axvline(boot_results['accuracy_ci'][1], color='orange', \n",
    "               linewidth=2, linestyle='--')\n",
    "axes[0].set_xlabel('Accuracy')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Bootstrap Distribution: Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# F1-Score distribution\n",
    "axes[1].hist(boot_results['boot_f1s'], bins=30, alpha=0.7, \n",
    "            color='coral', edgecolor='black')\n",
    "axes[1].axvline(boot_results['f1_mean'], color='red', linewidth=2,\n",
    "               label=f\"Mean = {boot_results['f1_mean']:.4f}\")\n",
    "axes[1].axvline(boot_results['f1_ci'][0], color='orange', \n",
    "               linewidth=2, linestyle='--', label=f\"95% CI\")\n",
    "axes[1].axvline(boot_results['f1_ci'][1], color='orange', \n",
    "               linewidth=2, linestyle='--')\n",
    "axes[1].set_xlabel('F1-Score')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Bootstrap Distribution: F1-Score')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. McNemar's Test: Compare Models\n",
    "\n",
    "**Goal:** Test if differences between models are statistically significant\n",
    "\n",
    "**Comparisons:**\n",
    "1. Weighted Fusion vs Behavior Only\n",
    "2. Weighted Fusion vs Gaze Only  \n",
    "3. Weighted Fusion vs Average Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "\n",
    "def mcnemar_test(y_true, pred_model1, pred_model2, model1_name, model2_name):\n",
    "    \"\"\"\n",
    "    McNemar's test to compare two models.\n",
    "    \"\"\"\n",
    "    # Create contingency table\n",
    "    # Format: [[both_correct, model1_correct_model2_wrong],\n",
    "    #          [model1_wrong_model2_correct, both_wrong]]\n",
    "    \n",
    "    both_correct = np.sum((pred_model1 == y_true) & (pred_model2 == y_true))\n",
    "    model1_correct = np.sum((pred_model1 == y_true) & (pred_model2 != y_true))\n",
    "    model2_correct = np.sum((pred_model1 != y_true) & (pred_model2 == y_true))\n",
    "    both_wrong = np.sum((pred_model1 != y_true) & (pred_model2 != y_true))\n",
    "    \n",
    "    contingency_table = np.array([[both_correct, model1_correct],\n",
    "                                  [model2_correct, both_wrong]])\n",
    "    \n",
    "    # Run McNemar's test\n",
    "    result = mcnemar(contingency_table, exact=False, correction=True)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"McNemar's Test: {model1_name} vs {model2_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\nContingency Table:\")\n",
    "    print(f\"  Both correct: {both_correct}\")\n",
    "    print(f\"  Only {model1_name} correct: {model1_correct}\")\n",
    "    print(f\"  Only {model2_name} correct: {model2_correct}\")\n",
    "    print(f\"  Both wrong: {both_wrong}\")\n",
    "    print(f\"\\nTest statistic: {result.statistic:.4f}\")\n",
    "    print(f\"p-value: {result.pvalue:.4f}\")\n",
    "    \n",
    "    if result.pvalue < 0.001:\n",
    "        print(f\"✓ Difference is HIGHLY SIGNIFICANT (p < 0.001)\")\n",
    "    elif result.pvalue < 0.01:\n",
    "        print(f\"✓ Difference is VERY SIGNIFICANT (p < 0.01)\")\n",
    "    elif result.pvalue < 0.05:\n",
    "        print(f\"✓ Difference is SIGNIFICANT (p < 0.05)\")\n",
    "    else:\n",
    "        print(f\"✗ Difference is NOT SIGNIFICANT (p = {result.pvalue:.4f})\")\n",
    "    \n",
    "    return {\n",
    "        'statistic': result.statistic,\n",
    "        'pvalue': result.pvalue,\n",
    "        'contingency_table': contingency_table\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from different models\n",
    "# (You'll need to run the late fusion model first to get these)\n",
    "\n",
    "# Example - you'd load actual predictions from your fusion results\n",
    "print(\"Note: Load actual model predictions from late_fusion_model_PRE.ipynb results\")\n",
    "print(\"This is a template showing the methodology.\")\n",
    "\n",
    "# Placeholder for demonstration\n",
    "# y_true = fusion_results['y_true']\n",
    "# pred_weighted = fusion_results['weighted']['predictions']\n",
    "# pred_behavior = fusion_results['behavior']['predictions']\n",
    "# pred_gaze = fusion_results['gaze']['predictions']\n",
    "# pred_average = fusion_results['average']['predictions']\n",
    "\n",
    "# mcnemar_1 = mcnemar_test(y_true, pred_weighted, pred_behavior, \n",
    "#                         \"Weighted Fusion\", \"Behavior Only\")\n",
    "# mcnemar_2 = mcnemar_test(y_true, pred_weighted, pred_gaze,\n",
    "#                         \"Weighted Fusion\", \"Gaze Only\")\n",
    "# mcnemar_3 = mcnemar_test(y_true, pred_weighted, pred_average,\n",
    "#                         \"Weighted Fusion\", \"Average Fusion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Table: All Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "summary_df = pd.DataFrame({\n",
    "    'Test': [\n",
    "        'Permutation Test',\n",
    "        'Bootstrap CI (Accuracy)',\n",
    "        'Bootstrap CI (F1-Score)'\n",
    "    ],\n",
    "    'Result': [\n",
    "        f\"{perm_results['true_accuracy']:.4f} (p = {perm_results['p_value']:.4f})\",\n",
    "        f\"{boot_results['accuracy_mean']:.4f} ({boot_results['accuracy_ci'][0]:.4f}-{boot_results['accuracy_ci'][1]:.4f})\",\n",
    "        f\"{boot_results['f1_mean']:.4f} ({boot_results['f1_ci'][0]:.4f}-{boot_results['f1_ci'][1]:.4f})\"\n",
    "    ],\n",
    "    'Interpretation': [\n",
    "        'Significantly > chance' if perm_results['p_value'] < 0.05 else 'Not significant',\n",
    "        '95% confidence interval',\n",
    "        '95% confidence interval'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STATISTICAL TESTING SUMMARY (PRE-DECISION)\")\n",
    "print(\"=\"*70)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save summary\n",
    "summary_df.to_csv('../../data/results/analysis_outputs_PRE/statistical_testing_summary_PRE.csv', index=False)\n",
    "print(\"\\n✓ Saved summary to: statistical_testing_summary_PRE.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results\n",
    "stat_results = {\n",
    "    'permutation_test': perm_results,\n",
    "    'bootstrap_ci': boot_results,\n",
    "    'summary': summary_df\n",
    "}\n",
    "\n",
    "with open('../../data/results/analysis_outputs_PRE/statistical_testing_results_PRE.pkl', 'wb') as f:\n",
    "    pickle.dump(stat_results, f)\n",
    "\n",
    "print(\"✓ Saved all statistical test results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "liinc_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
