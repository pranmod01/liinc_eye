{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Significance Testing\n",
    "\n",
    "This notebook performs rigorous statistical tests to validate that our model performance is:\n",
    "1. **Significantly better than chance** (permutation test)\n",
    "2. **Reliable** (bootstrap confidence intervals)\n",
    "3. **Significantly different between models** (McNemar's test)\n",
    "\n",
    "**Parameterized**: Set `TIMEFRAME` to run PRE or POST analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STATISTICAL SIGNIFICANCE TESTING: PRE-DECISION PERIOD\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION: Set timeframe for analysis\n",
    "# ============================================================================\n",
    "TIMEFRAME = 'PRE'  # Options: 'PRE', 'POST'\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"STATISTICAL SIGNIFICANCE TESTING: {TIMEFRAME}-DECISION PERIOD\")\n",
    "print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Decision Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trials: 12511\n",
      "Subjects: 97\n",
      "Outcome distribution: {1: 8238, 0: 4273}\n",
      "\n",
      "Class balance: 34.2% / 65.8%\n",
      "\n",
      "⚠️  NOTE: Using class_weight='balanced' to handle class imbalance\n",
      "   This will give different results than other notebooks without class balancing\n"
     ]
    }
   ],
   "source": [
    "# Load features\n",
    "with open(f'../../data/results/features_{TIMEFRAME}/extracted_features_{TIMEFRAME}.pkl', 'rb') as f:\n",
    "    feature_data = pickle.load(f)\n",
    "\n",
    "data = feature_data['merged_df']\n",
    "\n",
    "print(f\"Total trials: {len(data)}\")\n",
    "print(f\"Subjects: {data['subject_id'].nunique()}\")\n",
    "print(f\"Outcome distribution: {data['outcome'].value_counts().to_dict()}\")\n",
    "print(f\"\\nClass balance: {data['outcome'].value_counts()[0]/len(data):.1%} / {data['outcome'].value_counts()[1]/len(data):.1%}\")\n",
    "print(f\"\\n⚠️  NOTE: Using class_weight='balanced' to handle class imbalance\")\n",
    "print(f\"   This will give different results than other notebooks without class balancing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Permutation Test: Better Than Chance?\n",
    "\n",
    "**Null hypothesis:** Model accuracy = 50% (chance)\n",
    "\n",
    "**Method:** Shuffle labels 1000 times, recompute accuracy, compare to observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_test(X, y, subjects, n_permutations=1000, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Optimized permutation test to assess if accuracy is significantly > chance.\n",
    "    \n",
    "    Uses class_weight='balanced' to handle class imbalance properly.\n",
    "    Tests against 50% random chance baseline.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Permutation Test: {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Get true accuracy using LOSO CV\n",
    "    logo = LeaveOneGroupOut()\n",
    "    true_accs = []\n",
    "    \n",
    "    print(\"Computing true accuracy with class_weight='balanced'...\")\n",
    "    for train_idx, test_idx in tqdm(logo.split(X, y, subjects), \n",
    "                                     total=len(np.unique(subjects)),\n",
    "                                     desc=\"True LOSO folds\"):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        # Use class_weight='balanced' to handle imbalance\n",
    "        model = RandomForestClassifier(n_estimators=50, max_depth=5, \n",
    "                                      min_samples_split=10, min_samples_leaf=5,\n",
    "                                      class_weight='balanced',\n",
    "                                      random_state=42, n_jobs=1)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        true_accs.append(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    true_accuracy = np.mean(true_accs)\n",
    "    print(f\"True Accuracy: {true_accuracy:.4f}\")\n",
    "    \n",
    "    # Permutation test: shuffle labels to break feature-label relationship\n",
    "    print(f\"\\nRunning {n_permutations} permutations...\")\n",
    "    print(\"Testing against 50% chance baseline (with balanced classes)\")\n",
    "    \n",
    "    null_accs = []\n",
    "    \n",
    "    for i in tqdm(range(n_permutations), desc=\"Permutations\"):\n",
    "        # Randomly shuffle ALL labels (breaks feature-label relationship)\n",
    "        y_shuffled = np.random.permutation(y)\n",
    "        \n",
    "        # Sample subset of subjects for speed\n",
    "        unique_subjects = np.unique(subjects)\n",
    "        sample_size = min(20, len(unique_subjects))\n",
    "        sampled_subjects = np.random.choice(unique_subjects, size=sample_size, replace=False)\n",
    "        sampled_mask = np.isin(subjects, sampled_subjects)\n",
    "        \n",
    "        X_sample = X[sampled_mask]\n",
    "        y_sample = y_shuffled[sampled_mask]\n",
    "        subjects_sample = subjects[sampled_mask]\n",
    "        \n",
    "        # Run LOSO CV with shuffled labels\n",
    "        perm_accs = []\n",
    "        for train_idx, test_idx in logo.split(X_sample, y_sample, subjects_sample):\n",
    "            X_train, X_test = X_sample[train_idx], X_sample[test_idx]\n",
    "            y_train, y_test = y_sample[train_idx], y_sample[test_idx]\n",
    "            \n",
    "            # Use class_weight='balanced'\n",
    "            model = RandomForestClassifier(n_estimators=50, max_depth=5,\n",
    "                                          min_samples_split=10, min_samples_leaf=5,\n",
    "                                          class_weight='balanced',\n",
    "                                          random_state=i, n_jobs=1)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            perm_accs.append(accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        null_accs.append(np.mean(perm_accs))\n",
    "    \n",
    "    # Compute p-value\n",
    "    p_value = np.mean(np.array(null_accs) >= true_accuracy)\n",
    "    \n",
    "    print(f\"\\nNull Distribution Mean: {np.mean(null_accs):.4f} (should be ~50%)\")\n",
    "    print(f\"Null Distribution Std: {np.std(null_accs):.4f}\")\n",
    "    print(f\"True Accuracy: {true_accuracy:.4f}\")\n",
    "    print(f\"\\np-value: {p_value:.4f}\")\n",
    "    \n",
    "    if p_value < 0.001:\n",
    "        print(f\"✓ {model_name} is HIGHLY SIGNIFICANT (p < 0.001)\")\n",
    "    elif p_value < 0.01:\n",
    "        print(f\"✓ {model_name} is VERY SIGNIFICANT (p < 0.01)\")\n",
    "    elif p_value < 0.05:\n",
    "        print(f\"✓ {model_name} is SIGNIFICANT (p < 0.05)\")\n",
    "    else:\n",
    "        print(f\"✗ {model_name} is NOT SIGNIFICANT (p = {p_value:.4f})\")\n",
    "    \n",
    "    return {\n",
    "        'true_accuracy': true_accuracy,\n",
    "        'null_mean': np.mean(null_accs),\n",
    "        'null_std': np.std(null_accs),\n",
    "        'p_value': p_value,\n",
    "        'null_distribution': null_accs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physiology features: 13/13\n",
      "Gaze features: 20/20\n",
      "Behavior features: 7/7\n",
      "\n",
      "Dataset shapes:\n",
      "  Physiology: (12511, 13)\n",
      "  Gaze: (12511, 20)\n",
      "  Behavior: (12511, 7)\n",
      "  Physiology + Gaze: (12511, 33)\n",
      "  All features: (12511, 40)\n"
     ]
    }
   ],
   "source": [
    "# Define feature groups\n",
    "physiology_features = [\n",
    "    'pupil_mean_pre', 'pupil_std_pre', 'pupil_slope_pre', 'time_to_peak_pre', \n",
    "    'pupil_cv_pre', 'pupil_velocity_mean_pre', 'pupil_max_dilation_rate_pre',\n",
    "    'pupil_max_constriction_rate_pre', 'pupil_acceleration_std_pre',\n",
    "    'pct_time_dilating_pre', 'num_dilation_peaks_pre', 'eye_asymmetry_pre', \n",
    "    'eye_asymmetry_std_pre'\n",
    "]\n",
    "\n",
    "gaze_features = [\n",
    "    'gaze_valid_pct', 'gaze_x_mean', 'gaze_x_std', 'gaze_y_mean', 'gaze_y_std',\n",
    "    'screen_x_mean', 'screen_x_std', 'screen_y_mean', 'screen_y_std',\n",
    "    'gaze_velocity_mean', 'gaze_velocity_std', 'gaze_velocity_max',\n",
    "    'gaze_acceleration_mean', 'gaze_acceleration_std',\n",
    "    'fixation_ratio', 'saccade_ratio', 'saccade_count',\n",
    "    'gaze_dispersion_x', 'gaze_dispersion_y', 'gaze_path_length'\n",
    "]\n",
    "\n",
    "behavior_features = [\n",
    "    'reaction_time', 'decision_time', 'ev_difference',\n",
    "    'invest_variance', 'ambiguity', 'condition_social', 'risk_premium'\n",
    "]\n",
    "\n",
    "# Filter available features\n",
    "available_physiology = [f for f in physiology_features if f in data.columns]\n",
    "available_gaze = [f for f in gaze_features if f in data.columns]\n",
    "available_behavior = [f for f in behavior_features if f in data.columns]\n",
    "\n",
    "print(f\"Physiology features: {len(available_physiology)}/{len(physiology_features)}\")\n",
    "print(f\"Gaze features: {len(available_gaze)}/{len(gaze_features)}\")\n",
    "print(f\"Behavior features: {len(available_behavior)}/{len(behavior_features)}\")\n",
    "\n",
    "# Prepare data for all modalities\n",
    "X_physiology = SimpleImputer(strategy='mean').fit_transform(data[available_physiology])\n",
    "X_gaze = SimpleImputer(strategy='mean').fit_transform(data[available_gaze])\n",
    "X_behavior = SimpleImputer(strategy='mean').fit_transform(data[available_behavior])\n",
    "X_phys_gaze = SimpleImputer(strategy='mean').fit_transform(data[available_physiology + available_gaze])\n",
    "X_all = SimpleImputer(strategy='mean').fit_transform(data[available_physiology + available_gaze + available_behavior])\n",
    "\n",
    "y = data['outcome'].values\n",
    "subjects = data['subject_id'].values\n",
    "\n",
    "print(f\"\\nDataset shapes:\")\n",
    "print(f\"  Physiology: {X_physiology.shape}\")\n",
    "print(f\"  Gaze: {X_gaze.shape}\")\n",
    "print(f\"  Behavior: {X_behavior.shape}\")\n",
    "print(f\"  Physiology + Gaze: {X_phys_gaze.shape}\")\n",
    "print(f\"  All features: {X_all.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RUNNING PERMUTATION TESTS FOR ALL MODALITIES\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Permutation Test: Physiology (PRE)\n",
      "======================================================================\n",
      "Computing true accuracy with class_weight='balanced'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True LOSO folds: 100%|██████████| 97/97 [01:33<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Accuracy: 0.5200\n",
      "\n",
      "Running 1000 permutations...\n",
      "Testing against 50% chance baseline (with balanced classes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Permutations:  15%|█▌        | 151/1000 [08:25<50:37,  3.58s/it]  "
     ]
    }
   ],
   "source": [
    "# Run permutation tests on all modalities\n",
    "print(\"=\"*70)\n",
    "print(\"RUNNING PERMUTATION TESTS FOR ALL MODALITIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "perm_results = {}\n",
    "\n",
    "perm_results['physiology'] = permutation_test(\n",
    "    X_physiology, y, subjects, \n",
    "    n_permutations=1000, \n",
    "    model_name=f\"Physiology ({TIMEFRAME})\"\n",
    ")\n",
    "\n",
    "perm_results['gaze'] = permutation_test(\n",
    "    X_gaze, y, subjects, \n",
    "    n_permutations=1000, \n",
    "    model_name=f\"Gaze ({TIMEFRAME})\"\n",
    ")\n",
    "\n",
    "perm_results['behavior'] = permutation_test(\n",
    "    X_behavior, y, subjects, \n",
    "    n_permutations=1000, \n",
    "    model_name=f\"Behavior ({TIMEFRAME})\"\n",
    ")\n",
    "\n",
    "perm_results['phys_gaze'] = permutation_test(\n",
    "    X_phys_gaze, y, subjects, \n",
    "    n_permutations=1000, \n",
    "    model_name=f\"Physiology + Gaze ({TIMEFRAME})\"\n",
    ")\n",
    "\n",
    "perm_results['all'] = permutation_test(\n",
    "    X_all, y, subjects, \n",
    "    n_permutations=1000, \n",
    "    model_name=f\"All Features ({TIMEFRAME})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize permutation tests for all modalities\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "modalities = ['physiology', 'gaze', 'behavior', 'phys_gaze', 'all']\n",
    "titles = ['Physiology', 'Gaze', 'Behavior', 'Physiology + Gaze', 'All Features']\n",
    "\n",
    "for idx, (modality, title) in enumerate(zip(modalities, titles)):\n",
    "    ax = axes[idx]\n",
    "    result = perm_results[modality]\n",
    "    \n",
    "    ax.hist(result['null_distribution'], bins=50, alpha=0.7, \n",
    "            color='gray', edgecolor='black', label='Null Distribution')\n",
    "    ax.axvline(result['true_accuracy'], color='red', linewidth=3, \n",
    "              label=f\"True Acc = {result['true_accuracy']:.4f}\")\n",
    "    ax.axvline(result['null_mean'], color='blue', linewidth=2, \n",
    "              linestyle='--', label=f\"Null Mean = {result['null_mean']:.4f}\")\n",
    "    ax.set_xlabel('Accuracy')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f\"{title}\\np-value = {result['p_value']:.4f}\")\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Hide the 6th subplot\n",
    "axes[5].axis('off')\n",
    "\n",
    "plt.suptitle(f'Permutation Tests - All Modalities ({TIMEFRAME}-decision)', fontsize=14, y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'../../data/results/analysis_outputs_{TIMEFRAME}/figures/permutation_tests_all_{TIMEFRAME}.png', \n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print summary table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PERMUTATION TEST SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "perm_summary = pd.DataFrame({\n",
    "    'Model': titles,\n",
    "    'True Accuracy': [perm_results[m]['true_accuracy'] for m in modalities],\n",
    "    'Null Mean': [perm_results[m]['null_mean'] for m in modalities],\n",
    "    'p-value': [perm_results[m]['p_value'] for m in modalities],\n",
    "    'Significant': ['Yes' if perm_results[m]['p_value'] < 0.05 else 'No' for m in modalities]\n",
    "})\n",
    "print(perm_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bootstrap Confidence Intervals\n",
    "\n",
    "**Goal:** Estimate 95% CI for accuracy and F1-score\n",
    "\n",
    "**Method:** Resample subjects with replacement 1000 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_ci(X, y, subjects, n_bootstrap=1000, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Optimized bootstrap confidence intervals for accuracy and F1-score.\n",
    "    \n",
    "    Uses class_weight='balanced' to handle class imbalance.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Bootstrap Confidence Intervals: {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    unique_subjects = np.unique(subjects)\n",
    "    n_subjects = len(unique_subjects)\n",
    "    \n",
    "    boot_accs = []\n",
    "    boot_f1s = []\n",
    "    \n",
    "    # Sample ~30 subjects per bootstrap iteration for speed\n",
    "    sample_size = min(30, n_subjects)\n",
    "    \n",
    "    print(f\"Bootstrap strategy: Sampling {sample_size}/{n_subjects} subjects per iteration\")\n",
    "    print(f\"Using class_weight='balanced' in all models\")\n",
    "    \n",
    "    for i in tqdm(range(n_bootstrap), desc=\"Bootstrap iterations\"):\n",
    "        # Resample subjects with replacement\n",
    "        boot_subjects = np.random.choice(unique_subjects, size=sample_size, replace=True)\n",
    "        \n",
    "        # Create bootstrap sample\n",
    "        boot_mask = np.isin(subjects, boot_subjects)\n",
    "        X_boot = X[boot_mask]\n",
    "        y_boot = y[boot_mask]\n",
    "        subjects_boot = subjects[boot_mask]\n",
    "        \n",
    "        # Skip if we don't have enough subjects\n",
    "        if len(np.unique(subjects_boot)) < 5:\n",
    "            continue\n",
    "        \n",
    "        # LOSO CV on bootstrap sample\n",
    "        logo = LeaveOneGroupOut()\n",
    "        fold_accs = []\n",
    "        fold_f1s = []\n",
    "        \n",
    "        for train_idx, test_idx in logo.split(X_boot, y_boot, subjects_boot):\n",
    "            X_train, X_test = X_boot[train_idx], X_boot[test_idx]\n",
    "            y_train, y_test = y_boot[train_idx], y_boot[test_idx]\n",
    "            \n",
    "            # Skip folds with single class\n",
    "            if len(np.unique(y_train)) < 2 or len(np.unique(y_test)) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Use class_weight='balanced'\n",
    "            model = RandomForestClassifier(n_estimators=50, max_depth=5,\n",
    "                                          min_samples_split=10, min_samples_leaf=5,\n",
    "                                          class_weight='balanced',\n",
    "                                          random_state=i, n_jobs=1)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            fold_accs.append(accuracy_score(y_test, y_pred))\n",
    "            fold_f1s.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "        \n",
    "        # Only add if we got valid results\n",
    "        if len(fold_accs) > 0:\n",
    "            boot_accs.append(np.mean(fold_accs))\n",
    "            boot_f1s.append(np.mean(fold_f1s))\n",
    "    \n",
    "    # Compute 95% CI\n",
    "    acc_ci = np.percentile(boot_accs, [2.5, 97.5])\n",
    "    f1_ci = np.percentile(boot_f1s, [2.5, 97.5])\n",
    "    \n",
    "    print(f\"\\nCompleted {len(boot_accs)} valid bootstrap iterations\")\n",
    "    print(f\"Accuracy: {np.mean(boot_accs):.4f} (95% CI: {acc_ci[0]:.4f}-{acc_ci[1]:.4f})\")\n",
    "    print(f\"F1-Score: {np.mean(boot_f1s):.4f} (95% CI: {f1_ci[0]:.4f}-{f1_ci[1]:.4f})\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy_mean': np.mean(boot_accs),\n",
    "        'accuracy_ci': acc_ci,\n",
    "        'f1_mean': np.mean(boot_f1s),\n",
    "        'f1_ci': f1_ci,\n",
    "        'boot_accs': boot_accs,\n",
    "        'boot_f1s': boot_f1s\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run bootstrap for all modalities\n",
    "print(\"=\"*70)\n",
    "print(\"RUNNING BOOTSTRAP CONFIDENCE INTERVALS FOR ALL MODALITIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "boot_results = {}\n",
    "\n",
    "boot_results['physiology'] = bootstrap_ci(\n",
    "    X_physiology, y, subjects, \n",
    "    n_bootstrap=1000, \n",
    "    model_name=f\"Physiology ({TIMEFRAME})\"\n",
    ")\n",
    "\n",
    "boot_results['gaze'] = bootstrap_ci(\n",
    "    X_gaze, y, subjects, \n",
    "    n_bootstrap=1000, \n",
    "    model_name=f\"Gaze ({TIMEFRAME})\"\n",
    ")\n",
    "\n",
    "boot_results['behavior'] = bootstrap_ci(\n",
    "    X_behavior, y, subjects, \n",
    "    n_bootstrap=1000, \n",
    "    model_name=f\"Behavior ({TIMEFRAME})\"\n",
    ")\n",
    "\n",
    "boot_results['phys_gaze'] = bootstrap_ci(\n",
    "    X_phys_gaze, y, subjects, \n",
    "    n_bootstrap=1000, \n",
    "    model_name=f\"Physiology + Gaze ({TIMEFRAME})\"\n",
    ")\n",
    "\n",
    "boot_results['all'] = bootstrap_ci(\n",
    "    X_all, y, subjects, \n",
    "    n_bootstrap=1000, \n",
    "    model_name=f\"All Features ({TIMEFRAME})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize bootstrap confidence intervals\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "modalities = ['physiology', 'gaze', 'behavior', 'phys_gaze', 'all']\n",
    "titles = ['Physiology', 'Gaze', 'Behavior', 'Phys+Gaze', 'All Features']\n",
    "colors = ['steelblue', 'darkorange', 'forestgreen', 'purple', 'crimson']\n",
    "\n",
    "# Accuracy plot\n",
    "acc_means = [boot_results[m]['accuracy_mean'] for m in modalities]\n",
    "acc_cis = [boot_results[m]['accuracy_ci'] for m in modalities]\n",
    "acc_errors = [[acc_means[i] - acc_cis[i][0], acc_cis[i][1] - acc_means[i]] for i in range(len(modalities))]\n",
    "\n",
    "ax1.barh(titles, acc_means, xerr=np.array(acc_errors).T, color=colors, alpha=0.7, capsize=5)\n",
    "ax1.set_xlabel('Accuracy')\n",
    "ax1.set_title(f'Bootstrap 95% CI - Accuracy ({TIMEFRAME}-decision)')\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "ax1.axvline(0.5, color='black', linestyle='--', linewidth=1, alpha=0.5, label='Chance')\n",
    "ax1.legend()\n",
    "\n",
    "# F1-score plot\n",
    "f1_means = [boot_results[m]['f1_mean'] for m in modalities]\n",
    "f1_cis = [boot_results[m]['f1_ci'] for m in modalities]\n",
    "f1_errors = [[f1_means[i] - f1_cis[i][0], f1_cis[i][1] - f1_means[i]] for i in range(len(modalities))]\n",
    "\n",
    "ax2.barh(titles, f1_means, xerr=np.array(f1_errors).T, color=colors, alpha=0.7, capsize=5)\n",
    "ax2.set_xlabel('F1-Score (weighted)')\n",
    "ax2.set_title(f'Bootstrap 95% CI - F1-Score ({TIMEFRAME}-decision)')\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'../../data/results/analysis_outputs_{TIMEFRAME}/figures/bootstrap_ci_all_{TIMEFRAME}.png', \n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print summary table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BOOTSTRAP CONFIDENCE INTERVALS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "boot_summary = pd.DataFrame({\n",
    "    'Model': titles,\n",
    "    'Accuracy': [f\"{boot_results[m]['accuracy_mean']:.4f}\" for m in modalities],\n",
    "    'Accuracy 95% CI': [f\"({boot_results[m]['accuracy_ci'][0]:.4f}-{boot_results[m]['accuracy_ci'][1]:.4f})\" for m in modalities],\n",
    "    'F1-Score': [f\"{boot_results[m]['f1_mean']:.4f}\" for m in modalities],\n",
    "    'F1 95% CI': [f\"({boot_results[m]['f1_ci'][0]:.4f}-{boot_results[m]['f1_ci'][1]:.4f})\" for m in modalities]\n",
    "})\n",
    "print(boot_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. McNemar's Test: Compare Models\n",
    "\n",
    "**Goal:** Test if differences between models are statistically significant\n",
    "\n",
    "**Comparisons:**\n",
    "1. Combined vs Behavior Only\n",
    "2. Combined vs Gaze Only  \n",
    "3. Combined vs Physiology Only\n",
    "4. Behavior vs Gaze\n",
    "5. Behavior vs Physiology\n",
    "6. Gaze vs Physiology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "\n",
    "def mcnemar_test(y_true, pred_model1, pred_model2, model1_name, model2_name):\n",
    "    \"\"\"\n",
    "    McNemar's test to compare two models.\n",
    "    \n",
    "    Tests whether the two models make significantly different errors.\n",
    "    Null hypothesis: Both models have the same error rate.\n",
    "    \"\"\"\n",
    "    # Create contingency table\n",
    "    # Format: [[both_correct, model1_correct_model2_wrong],\n",
    "    #          [model1_wrong_model2_correct, both_wrong]]\n",
    "    \n",
    "    both_correct = np.sum((pred_model1 == y_true) & (pred_model2 == y_true))\n",
    "    model1_correct = np.sum((pred_model1 == y_true) & (pred_model2 != y_true))\n",
    "    model2_correct = np.sum((pred_model1 != y_true) & (pred_model2 == y_true))\n",
    "    both_wrong = np.sum((pred_model1 != y_true) & (pred_model2 != y_true))\n",
    "    \n",
    "    contingency_table = np.array([[both_correct, model1_correct],\n",
    "                                  [model2_correct, both_wrong]])\n",
    "    \n",
    "    # Run McNemar's test\n",
    "    result = mcnemar(contingency_table, exact=False, correction=True)\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    acc1 = accuracy_score(y_true, pred_model1)\n",
    "    acc2 = accuracy_score(y_true, pred_model2)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"McNemar's Test: {model1_name} vs {model2_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\nModel Accuracies:\")\n",
    "    print(f\"  {model1_name}: {acc1:.4f}\")\n",
    "    print(f\"  {model2_name}: {acc2:.4f}\")\n",
    "    print(f\"  Difference: {abs(acc1 - acc2):.4f}\")\n",
    "    print(f\"\\nContingency Table:\")\n",
    "    print(f\"  Both correct: {both_correct}\")\n",
    "    print(f\"  Only {model1_name} correct: {model1_correct}\")\n",
    "    print(f\"  Only {model2_name} correct: {model2_correct}\")\n",
    "    print(f\"  Both wrong: {both_wrong}\")\n",
    "    print(f\"\\nMcNemar Test Results:\")\n",
    "    print(f\"  Test statistic: {result.statistic:.4f}\")\n",
    "    print(f\"  p-value: {result.pvalue:.4f}\")\n",
    "    \n",
    "    if result.pvalue < 0.001:\n",
    "        print(f\"✓ Difference is HIGHLY SIGNIFICANT (p < 0.001)\")\n",
    "    elif result.pvalue < 0.01:\n",
    "        print(f\"✓ Difference is VERY SIGNIFICANT (p < 0.01)\")\n",
    "    elif result.pvalue < 0.05:\n",
    "        print(f\"✓ Difference is SIGNIFICANT (p < 0.05)\")\n",
    "    else:\n",
    "        print(f\"✗ Difference is NOT SIGNIFICANT (p = {result.pvalue:.4f})\")\n",
    "    \n",
    "    return {\n",
    "        'statistic': result.statistic,\n",
    "        'pvalue': result.pvalue,\n",
    "        'contingency_table': contingency_table,\n",
    "        'acc1': acc1,\n",
    "        'acc2': acc2\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions for All Models\n",
    "\n",
    "We need predictions from all models to run McNemar's test comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions from all models using LOSO CV\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "def get_model_predictions(X, y, subjects):\n",
    "    \"\"\"Get predictions from LOSO cross-validation with class_weight='balanced'.\"\"\"\n",
    "    logo = LeaveOneGroupOut()\n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=5, \n",
    "                                  min_samples_split=10, min_samples_leaf=5,\n",
    "                                  class_weight='balanced',\n",
    "                                  random_state=42, n_jobs=-1)\n",
    "    \n",
    "    preds_all = []\n",
    "    y_true_all = []\n",
    "    \n",
    "    for train_idx, test_idx in logo.split(X, y, subjects):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        preds_all.extend(y_pred)\n",
    "        y_true_all.extend(y_test)\n",
    "    \n",
    "    return np.array(preds_all), np.array(y_true_all)\n",
    "\n",
    "print(\"Generating predictions for all models...\")\n",
    "print(\"⚠️  Using class_weight='balanced' (different from other notebooks!)\")\n",
    "print(\"This will take several minutes...\\n\")\n",
    "\n",
    "# Generate predictions for all modalities\n",
    "pred_physio, y_true = get_model_predictions(X_physiology, y, subjects)\n",
    "print(f\"✓ Physiology model - Accuracy: {accuracy_score(y_true, pred_physio):.4f}\")\n",
    "\n",
    "pred_gaze, _ = get_model_predictions(X_gaze, y, subjects)\n",
    "print(f\"✓ Gaze model - Accuracy: {accuracy_score(y_true, pred_gaze):.4f}\")\n",
    "\n",
    "pred_behavior, _ = get_model_predictions(X_behavior, y, subjects)\n",
    "print(f\"✓ Behavior model - Accuracy: {accuracy_score(y_true, pred_behavior):.4f}\")\n",
    "\n",
    "pred_phys_gaze, _ = get_model_predictions(X_phys_gaze, y, subjects)\n",
    "print(f\"✓ Physiology + Gaze model - Accuracy: {accuracy_score(y_true, pred_phys_gaze):.4f}\")\n",
    "\n",
    "pred_all, _ = get_model_predictions(X_all, y, subjects)\n",
    "print(f\"✓ All features model - Accuracy: {accuracy_score(y_true, pred_all):.4f}\")\n",
    "\n",
    "print(\"\\n✓ All predictions generated!\")\n",
    "print(\"\\n⚠️  Remember to add class_weight='balanced' to all other modeling notebooks!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all pairwise comparisons\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MCNEMAR'S TESTS: PAIRWISE MODEL COMPARISONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Store all results\n",
    "mcnemar_results = {}\n",
    "\n",
    "# 1. All Features vs individual modalities\n",
    "mcnemar_results['all_vs_physio'] = mcnemar_test(\n",
    "    y_true, pred_all, pred_physio, \n",
    "    \"All Features\", \"Physiology\"\n",
    ")\n",
    "\n",
    "mcnemar_results['all_vs_gaze'] = mcnemar_test(\n",
    "    y_true, pred_all, pred_gaze,\n",
    "    \"All Features\", \"Gaze\"\n",
    ")\n",
    "\n",
    "mcnemar_results['all_vs_behavior'] = mcnemar_test(\n",
    "    y_true, pred_all, pred_behavior, \n",
    "    \"All Features\", \"Behavior\"\n",
    ")\n",
    "\n",
    "mcnemar_results['all_vs_phys_gaze'] = mcnemar_test(\n",
    "    y_true, pred_all, pred_phys_gaze,\n",
    "    \"All Features\", \"Physiology+Gaze\"\n",
    ")\n",
    "\n",
    "# 2. Physiology+Gaze vs individual modalities\n",
    "mcnemar_results['phys_gaze_vs_physio'] = mcnemar_test(\n",
    "    y_true, pred_phys_gaze, pred_physio,\n",
    "    \"Physiology+Gaze\", \"Physiology\"\n",
    ")\n",
    "\n",
    "mcnemar_results['phys_gaze_vs_gaze'] = mcnemar_test(\n",
    "    y_true, pred_phys_gaze, pred_gaze,\n",
    "    \"Physiology+Gaze\", \"Gaze\"\n",
    ")\n",
    "\n",
    "mcnemar_results['phys_gaze_vs_behavior'] = mcnemar_test(\n",
    "    y_true, pred_phys_gaze, pred_behavior,\n",
    "    \"Physiology+Gaze\", \"Behavior\"\n",
    ")\n",
    "\n",
    "# 3. Between individual modalities\n",
    "mcnemar_results['physio_vs_gaze'] = mcnemar_test(\n",
    "    y_true, pred_physio, pred_gaze,\n",
    "    \"Physiology\", \"Gaze\"\n",
    ")\n",
    "\n",
    "mcnemar_results['physio_vs_behavior'] = mcnemar_test(\n",
    "    y_true, pred_physio, pred_behavior,\n",
    "    \"Physiology\", \"Behavior\"\n",
    ")\n",
    "\n",
    "mcnemar_results['gaze_vs_behavior'] = mcnemar_test(\n",
    "    y_true, pred_gaze, pred_behavior,\n",
    "    \"Gaze\", \"Behavior\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table for McNemar's tests\n",
    "comparison_names = [\n",
    "    'All Features vs Physiology',\n",
    "    'All Features vs Gaze',\n",
    "    'All Features vs Behavior',\n",
    "    'All Features vs Physiology+Gaze',\n",
    "    'Physiology+Gaze vs Physiology',\n",
    "    'Physiology+Gaze vs Gaze',\n",
    "    'Physiology+Gaze vs Behavior',\n",
    "    'Physiology vs Gaze',\n",
    "    'Physiology vs Behavior',\n",
    "    'Gaze vs Behavior'\n",
    "]\n",
    "\n",
    "result_keys = [\n",
    "    'all_vs_physio', 'all_vs_gaze', 'all_vs_behavior', 'all_vs_phys_gaze',\n",
    "    'phys_gaze_vs_physio', 'phys_gaze_vs_gaze', 'phys_gaze_vs_behavior',\n",
    "    'physio_vs_gaze', 'physio_vs_behavior', 'gaze_vs_behavior'\n",
    "]\n",
    "\n",
    "mcnemar_summary = pd.DataFrame({\n",
    "    'Comparison': comparison_names,\n",
    "    'Accuracy_1': [mcnemar_results[k]['acc1'] for k in result_keys],\n",
    "    'Accuracy_2': [mcnemar_results[k]['acc2'] for k in result_keys],\n",
    "    'Difference': [abs(mcnemar_results[k]['acc1'] - mcnemar_results[k]['acc2']) for k in result_keys],\n",
    "    'p_value': [mcnemar_results[k]['pvalue'] for k in result_keys],\n",
    "    'Significant': ['Yes' if mcnemar_results[k]['pvalue'] < 0.05 else 'No' for k in result_keys]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MCNEMAR'S TEST SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(mcnemar_summary.to_string(index=False))\n",
    "\n",
    "# Save McNemar results\n",
    "mcnemar_summary.to_csv(f'../../data/results/analysis_outputs_{TIMEFRAME}/mcnemar_test_summary_{TIMEFRAME}.csv', index=False)\n",
    "print(f\"\\n✓ Saved McNemar's test summary to analysis_outputs_{TIMEFRAME}/mcnemar_test_summary_{TIMEFRAME}.csv\")\n",
    "\n",
    "# Visualize p-values\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "colors = ['green' if p < 0.05 else 'red' for p in mcnemar_summary['p_value']]\n",
    "bars = ax.barh(mcnemar_summary['Comparison'], mcnemar_summary['p_value'], color=colors, alpha=0.7)\n",
    "ax.axvline(0.05, color='black', linestyle='--', linewidth=2, label='p = 0.05 threshold')\n",
    "ax.set_xlabel('p-value')\n",
    "ax.set_title(f\"McNemar's Test: Pairwise Model Comparisons ({TIMEFRAME}-decision)\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'../../data/results/analysis_outputs_{TIMEFRAME}/figures/mcnemar_pvalues_{TIMEFRAME}.png', \n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Table: All Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary table\n",
    "modalities = ['physiology', 'gaze', 'behavior', 'phys_gaze', 'all']\n",
    "modality_names = ['Physiology', 'Gaze', 'Behavior', 'Physiology+Gaze', 'All Features']\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "# Add permutation test results\n",
    "for mod, name in zip(modalities, modality_names):\n",
    "    summary_rows.append({\n",
    "        'Test': f'Permutation Test ({name})',\n",
    "        'Result': f\"p = {perm_results[mod]['p_value']:.4f}\",\n",
    "        'Interpretation': 'Significantly > chance' if perm_results[mod]['p_value'] < 0.05 else 'Not significant'\n",
    "    })\n",
    "\n",
    "summary_rows.append({'Test': '', 'Result': '', 'Interpretation': ''})\n",
    "\n",
    "# Add bootstrap CI results\n",
    "for mod, name in zip(modalities, modality_names):\n",
    "    summary_rows.append({\n",
    "        'Test': f'Bootstrap CI - Accuracy ({name})',\n",
    "        'Result': f\"{boot_results[mod]['accuracy_mean']:.4f} ({boot_results[mod]['accuracy_ci'][0]:.4f}-{boot_results[mod]['accuracy_ci'][1]:.4f})\",\n",
    "        'Interpretation': '95% confidence interval'\n",
    "    })\n",
    "\n",
    "summary_rows.append({'Test': '', 'Result': '', 'Interpretation': ''})\n",
    "\n",
    "for mod, name in zip(modalities, modality_names):\n",
    "    summary_rows.append({\n",
    "        'Test': f'Bootstrap CI - F1-Score ({name})',\n",
    "        'Result': f\"{boot_results[mod]['f1_mean']:.4f} ({boot_results[mod]['f1_ci'][0]:.4f}-{boot_results[mod]['f1_ci'][1]:.4f})\",\n",
    "        'Interpretation': '95% confidence interval'\n",
    "    })\n",
    "\n",
    "summary_rows.append({'Test': '', 'Result': '', 'Interpretation': ''})\n",
    "\n",
    "# Add McNemar tests\n",
    "for comp_name, key in zip(comparison_names, result_keys):\n",
    "    summary_rows.append({\n",
    "        'Test': f\"McNemar: {comp_name}\",\n",
    "        'Result': f\"p = {mcnemar_results[key]['pvalue']:.4f}\",\n",
    "        'Interpretation': 'Significant' if mcnemar_results[key]['pvalue'] < 0.05 else 'Not significant'\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"COMPREHENSIVE STATISTICAL TESTING SUMMARY ({TIMEFRAME}-DECISION)\")\n",
    "print(\"=\"*70)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save comprehensive summary\n",
    "summary_df.to_csv(f'../../data/results/analysis_outputs_{TIMEFRAME}/statistical_testing_summary_{TIMEFRAME}.csv', index=False)\n",
    "print(f\"\\n✓ Saved comprehensive summary to: statistical_testing_summary_{TIMEFRAME}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results including all tests\n",
    "stat_results = {\n",
    "    'permutation_tests': perm_results,\n",
    "    'bootstrap_ci': boot_results,\n",
    "    'mcnemar_tests': mcnemar_results,\n",
    "    'summary': summary_df,\n",
    "    'mcnemar_summary': mcnemar_summary,\n",
    "    'perm_summary': perm_summary,\n",
    "    'boot_summary': boot_summary\n",
    "}\n",
    "\n",
    "with open(f'../../data/results/analysis_outputs_{TIMEFRAME}/statistical_testing_results_{TIMEFRAME}.pkl', 'wb') as f:\n",
    "    pickle.dump(stat_results, f)\n",
    "\n",
    "print(f\"✓ Saved all statistical test results to: statistical_testing_results_{TIMEFRAME}.pkl\")\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nAll results saved to: ../../data/results/analysis_outputs_{TIMEFRAME}/\")\n",
    "print(\"\\nGenerated files:\")\n",
    "print(f\"  - mcnemar_test_summary_{TIMEFRAME}.csv\")\n",
    "print(f\"  - statistical_testing_summary_{TIMEFRAME}.csv\")\n",
    "print(f\"  - statistical_testing_results_{TIMEFRAME}.pkl\")\n",
    "print(f\"  - figures/permutation_tests_all_{TIMEFRAME}.png\")\n",
    "print(f\"  - figures/bootstrap_ci_all_{TIMEFRAME}.png\")\n",
    "print(f\"  - figures/mcnemar_pvalues_{TIMEFRAME}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "liinc_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
