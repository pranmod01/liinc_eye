{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ce5f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                            roc_auc_score, roc_curve, precision_recall_curve,\n",
    "                            f1_score, accuracy_score)\n",
    "import statsmodels\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91e98dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD ALL SUBJECTS' DATA\n",
    "# =============================================================================\n",
    "# Update these paths to your data directories\n",
    "preprocessed_dir = Path('path/to/preprocessed/files')\n",
    "raw_dir = Path('path/to/raw/files')\n",
    "\n",
    "# Get all preprocessed JSON files\n",
    "preprocessed_files = list(preprocessed_dir.glob('preprocessing_*.json'))\n",
    "print(f\"Found {len(preprocessed_files)} preprocessed files\")\n",
    "\n",
    "# =============================================================================\n",
    "# CHOOSE BASELINE METHOD\n",
    "# =============================================================================\n",
    "baseline_method = 't3_stable_pre_decision'  # or 't0_initial_fixation', 't1_post_stabilization', 't2_early_post_stimulus'\n",
    "\n",
    "# =============================================================================\n",
    "# EXTRACT FEATURES FOR ALL SUBJECTS\n",
    "# =============================================================================\n",
    "all_physiology_features = []\n",
    "all_behavior_features = []\n",
    "all_outcomes = []\n",
    "all_subject_ids = []\n",
    "all_trial_ids = []\n",
    "\n",
    "for preprocessed_file in preprocessed_files:\n",
    "    \n",
    "    # Load preprocessed data\n",
    "    with open(preprocessed_file, 'r') as f:\n",
    "        preprocessed = json.load(f)\n",
    "    \n",
    "    subject_id = preprocessed['subject_id']\n",
    "    print(f\"\\nProcessing subject: {subject_id}\")\n",
    "    \n",
    "    # Find corresponding raw data file\n",
    "    # Adjust this pattern to match your raw file naming convention\n",
    "    raw_file = raw_dir / f\"{subject_id}.json\"  # or however your raw files are named\n",
    "    \n",
    "    if not raw_file.exists():\n",
    "        print(f\"  Warning: Raw file not found for {subject_id}, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    with open(raw_file, 'r') as f:\n",
    "        raw_data = json.load(f)\n",
    "    \n",
    "    subject_trial_count = 0\n",
    "    \n",
    "    for trial_id, trial_data in preprocessed['trials'].items():\n",
    "        \n",
    "        # Skip if preprocessing failed\n",
    "        if trial_data['preprocessing_status'] != 'success':\n",
    "            continue\n",
    "        \n",
    "        # Get the baseline-corrected pupil data for chosen method\n",
    "        method_data = trial_data['baseline_methods'][baseline_method]\n",
    "        \n",
    "        if method_data['status'] != 'success':\n",
    "            continue\n",
    "        \n",
    "        # Get raw trial data for behavioral features\n",
    "        trial_idx = int(trial_id.split('_')[1])  # Extract trial number from 'trial_0', 'trial_1', etc.\n",
    "        raw_trial = raw_data['trials'][trial_idx]\n",
    "        \n",
    "        # Skip trials that weren't submitted\n",
    "        if not raw_trial['submitted']:\n",
    "            continue\n",
    "        \n",
    "        # =============================================================================\n",
    "        # PHYSIOLOGY FEATURES (from preprocessed pupil data)\n",
    "        # =============================================================================\n",
    "        pupil_avg = np.array(method_data['pupil_avg_baselined'])\n",
    "        pupil_L = np.array(method_data['pupil_L_baselined'])\n",
    "        pupil_R = np.array(method_data['pupil_R_baselined'])\n",
    "        time = np.array(method_data['time_aligned'])\n",
    "        \n",
    "        # Remove NaN values\n",
    "        valid_mask = ~np.isnan(pupil_avg)\n",
    "        pupil_avg_clean = pupil_avg[valid_mask]\n",
    "        time_clean = time[valid_mask]\n",
    "        \n",
    "        if len(pupil_avg_clean) < 10:\n",
    "            continue\n",
    "        \n",
    "        # Calculate pupil dilation velocity (rate of change)\n",
    "        pupil_velocity = np.diff(pupil_avg_clean)\n",
    "        \n",
    "        # Extract physiology features\n",
    "        physiology_features = {\n",
    "            # Basic statistics\n",
    "            'pupil_mean': np.mean(pupil_avg_clean),\n",
    "            'pupil_std': np.std(pupil_avg_clean),\n",
    "            'pupil_min': np.min(pupil_avg_clean),\n",
    "            'pupil_max': np.max(pupil_avg_clean),\n",
    "            'pupil_range': np.max(pupil_avg_clean) - np.min(pupil_avg_clean),\n",
    "            \n",
    "            # Peak dilation metrics\n",
    "            'pupil_peak_dilation': np.max(pupil_avg_clean),\n",
    "            'time_to_peak': time_clean[np.argmax(pupil_avg_clean)],\n",
    "            \n",
    "            # Temporal dynamics\n",
    "            'pupil_slope': np.polyfit(time_clean, pupil_avg_clean, 1)[0] if len(time_clean) > 1 else 0,\n",
    "            'pupil_initial': np.mean(pupil_avg_clean[:10]),  # First 10 samples\n",
    "            'pupil_final': np.mean(pupil_avg_clean[-10:]),   # Last 10 samples\n",
    "            'pupil_change': np.mean(pupil_avg_clean[-10:]) - np.mean(pupil_avg_clean[:10]),\n",
    "            \n",
    "            # Variability metrics\n",
    "            'pupil_cv': np.std(pupil_avg_clean) / np.abs(np.mean(pupil_avg_clean)) if np.mean(pupil_avg_clean) != 0 else 0,\n",
    "            'pupil_velocity_mean': np.mean(np.abs(pupil_velocity)),\n",
    "            'pupil_velocity_max': np.max(np.abs(pupil_velocity)),\n",
    "            \n",
    "            # Baseline values\n",
    "            'baseline_L': method_data['baseline_L'],\n",
    "            'baseline_R': method_data['baseline_R'],\n",
    "            \n",
    "            # Eye asymmetry\n",
    "            'eye_asymmetry': np.mean(np.abs(pupil_L[valid_mask] - pupil_R[valid_mask])),\n",
    "        }\n",
    "        \n",
    "        # =============================================================================\n",
    "        # BEHAVIOR FEATURES (from raw trial data)\n",
    "        # =============================================================================\n",
    "        gamble_params = raw_trial['gamble parameters']\n",
    "        lct = raw_trial['lct']\n",
    "        \n",
    "        # Extract timing information\n",
    "        show_screen_time = None\n",
    "        submit_time = None\n",
    "        click_time = None\n",
    "        \n",
    "        for event in lct:\n",
    "            if 'show screen' in event['event']:\n",
    "                show_screen_time = event['time']\n",
    "            elif 'gamble clicked' in event['event']:\n",
    "                click_time = event['time']\n",
    "            elif 'submit' in event['event']:\n",
    "                submit_time = event['time']\n",
    "        \n",
    "        if show_screen_time is None or submit_time is None:\n",
    "            continue\n",
    "        \n",
    "        # Calculate timing metrics (convert to seconds)\n",
    "        reaction_time = (click_time - show_screen_time) / 1000 if click_time else np.nan\n",
    "        decision_time = (submit_time - show_screen_time) / 1000\n",
    "        \n",
    "        # Calculate expected values\n",
    "        invest_ev = (gamble_params['invest amount 1'] * gamble_params['invest probability 1'] + \n",
    "                     gamble_params['invest amount 2'] * gamble_params['invest probability 2'])\n",
    "        keep_ev = gamble_params['keep amount']\n",
    "        ev_difference = invest_ev - keep_ev\n",
    "        \n",
    "        # Choice information\n",
    "        num_choice_switches = len(raw_trial['choices'])\n",
    "        final_choice = raw_trial['choices'][-1] if len(raw_trial['choices']) > 0 else None\n",
    "        chose_invest = 1 if final_choice == 'INVEST' else 0\n",
    "        \n",
    "        behavior_features = {\n",
    "            # Timing\n",
    "            'reaction_time': reaction_time if not np.isnan(reaction_time) else decision_time,\n",
    "            'decision_time': decision_time,\n",
    "                        \n",
    "            # Gamble parameters\n",
    "            'keep_amount': gamble_params['keep amount'],\n",
    "            'invest_ev': invest_ev,\n",
    "            'keep_ev': keep_ev,\n",
    "            'ev_difference': ev_difference,\n",
    "            'ambiguity': gamble_params['ambiguity'],\n",
    "            'condition_social': 1 if gamble_params['condition'] == 'social' else 0,\n",
    "            \n",
    "            # Risk metrics\n",
    "            'invest_variance': ((gamble_params['invest amount 1'] - invest_ev)**2 * gamble_params['invest probability 1'] +\n",
    "                               (gamble_params['invest amount 2'] - invest_ev)**2 * gamble_params['invest probability 2']),\n",
    "        }\n",
    "        \n",
    "        # =============================================================================\n",
    "        # OUTCOME VARIABLE\n",
    "        # =============================================================================\n",
    "        outcome = chose_invest  # Predicting INVEST (1) vs KEEP (0)\n",
    "        \n",
    "        # Store everything\n",
    "        all_physiology_features.append(physiology_features)\n",
    "        all_behavior_features.append(behavior_features)\n",
    "        all_outcomes.append(outcome)\n",
    "        all_subject_ids.append(subject_id)\n",
    "        all_trial_ids.append(f\"{subject_id}_{trial_id}\")\n",
    "        \n",
    "        subject_trial_count += 1\n",
    "    \n",
    "    print(f\"  Extracted {subject_trial_count} valid trials\")\n",
    "\n",
    "# =============================================================================\n",
    "# CREATE DATAFRAMES\n",
    "# =============================================================================\n",
    "physiology_df = pd.DataFrame(all_physiology_features)\n",
    "behavior_df = pd.DataFrame(all_behavior_features)\n",
    "outcomes = np.array(all_outcomes)\n",
    "subjects = np.array(all_subject_ids)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"DATA SUMMARY (ALL SUBJECTS)\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total subjects: {len(np.unique(subjects))}\")\n",
    "print(f\"Total valid trials: {len(outcomes)}\")\n",
    "print(f\"Physiology features: {physiology_df.shape[1]}\")\n",
    "print(f\"Behavior features: {behavior_df.shape[1]}\")\n",
    "print(f\"\\nOutcome distribution:\")\n",
    "print(f\"  KEEP (0): {np.sum(outcomes == 0)} ({np.mean(outcomes == 0):.1%})\")\n",
    "print(f\"  INVEST (1): {np.sum(outcomes == 1)} ({np.mean(outcomes == 1):.1%})\")\n",
    "\n",
    "# Per-subject breakdown\n",
    "print(f\"\\nPer-subject trial counts:\")\n",
    "subject_counts = pd.DataFrame({'subject': subjects, 'outcome': outcomes})\n",
    "print(subject_counts.groupby('subject')['outcome'].agg(['count', 'mean']))\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values in physiology features:\")\n",
    "missing_physio = physiology_df.isnull().sum()\n",
    "if missing_physio.sum() > 0:\n",
    "    print(missing_physio[missing_physio > 0])\n",
    "else:\n",
    "    print(\"None!\")\n",
    "\n",
    "print(f\"\\nMissing values in behavior features:\")\n",
    "missing_behavior = behavior_df.isnull().sum()\n",
    "if missing_behavior.sum() > 0:\n",
    "    print(missing_behavior[missing_behavior > 0])\n",
    "else:\n",
    "    print(\"None!\")\n",
    "\n",
    "# =============================================================================\n",
    "# CHECK MULTICOLLINEARITY IN PHYSIOLOGY FEATURES\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"MULTICOLLINEARITY ANALYSIS - PHYSIOLOGY FEATURES\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Calculate correlation matrix\n",
    "physiology_corr = physiology_df.corr()\n",
    "\n",
    "# Find highly correlated pairs (>0.9)\n",
    "high_corr_pairs = []\n",
    "for i in range(len(physiology_corr.columns)):\n",
    "    for j in range(i+1, len(physiology_corr.columns)):\n",
    "        if abs(physiology_corr.iloc[i, j]) > 0.9:\n",
    "            high_corr_pairs.append({\n",
    "                'feature_1': physiology_corr.columns[i],\n",
    "                'feature_2': physiology_corr.columns[j],\n",
    "                'correlation': physiology_corr.iloc[i, j]\n",
    "            })\n",
    "\n",
    "print(f\"\\n⚠️  Highly correlated feature pairs (|r| > 0.9):\")\n",
    "for pair in high_corr_pairs:\n",
    "    print(f\"  {pair['feature_1']} <-> {pair['feature_2']}: {pair['correlation']:.3f}\")\n",
    "\n",
    "# Visualize correlation matrix\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(physiology_corr, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "ax.set_title('Physiology Features Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate VIF (Variance Inflation Factor) for multicollinearity\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "X_physio_for_vif = physiology_df.fillna(physiology_df.mean())  # VIF needs no NaN\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X_physio_for_vif.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_physio_for_vif.values, i) \n",
    "                   for i in range(len(X_physio_for_vif.columns))]\n",
    "vif_data = vif_data.sort_values('VIF', ascending=False)\n",
    "\n",
    "print(f\"\\nVariance Inflation Factor (VIF):\")\n",
    "print(vif_data)\n",
    "print(f\"\\nNote: VIF > 10 indicates high multicollinearity\")\n",
    "print(f\"      VIF > 5 indicates moderate multicollinearity\")\n",
    "\n",
    "# =============================================================================\n",
    "# TASK 2: PHYSIOLOGY → OUTCOME (Direct)\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"TASK 2: PHYSIOLOGY → OUTCOME (Direct)\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Prepare data\n",
    "X_physio = physiology_df.values\n",
    "y = outcomes\n",
    "\n",
    "# Check class balance\n",
    "print(f\"\\nClass balance check:\")\n",
    "print(f\"  Minimum class size: {min(np.bincount(y))}\")\n",
    "print(f\"  Class ratio: {np.mean(y):.3f}\")\n",
    "\n",
    "# Train-test split (stratified to maintain class balance)\n",
    "X_train_physio, X_test_physio, y_train, y_test = train_test_split(\n",
    "    X_physio, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining samples: {len(y_train)} (KEEP: {np.sum(y_train==0)}, INVEST: {np.sum(y_train==1)})\")\n",
    "print(f\"Test samples: {len(y_test)} (KEEP: {np.sum(y_test==0)}, INVEST: {np.sum(y_test==1)})\")\n",
    "\n",
    "# Train Random Forest classifier\n",
    "rf_physio = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    random_state=42, \n",
    "    max_depth=5,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    class_weight='balanced'  # Handle class imbalance\n",
    ")\n",
    "rf_physio.fit(X_train_physio, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_physio = rf_physio.predict(X_test_physio)\n",
    "y_prob_physio = rf_physio.predict_proba(X_test_physio)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "accuracy_physio = accuracy_score(y_test, y_pred_physio)\n",
    "print(f\"\\nAccuracy: {accuracy_physio:.3f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_physio, target_names=['KEEP', 'INVEST']))\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(rf_physio, X_physio, y, cv=5, scoring='accuracy')\n",
    "print(f\"\\n5-Fold Cross-Validation Accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std():.3f})\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance_physio = pd.DataFrame({\n",
    "    'feature': physiology_df.columns,\n",
    "    'importance': rf_physio.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Physiology Features:\")\n",
    "print(feature_importance_physio.head(10))\n",
    "\n",
    "# Plot feature importance\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "top_n = min(15, len(feature_importance_physio))\n",
    "ax.barh(range(top_n), feature_importance_physio['importance'][:top_n], color='steelblue', edgecolor='black')\n",
    "ax.set_yticks(range(top_n))\n",
    "ax.set_yticklabels(feature_importance_physio['feature'][:top_n])\n",
    "ax.set_xlabel('Importance', fontsize=12, fontweight='bold')\n",
    "ax.set_title(f'Top {top_n} Physiology Features for Outcome Prediction\\n(All Subjects Combined)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix\n",
    "cm_physio = confusion_matrix(y_test, y_pred_physio)\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "sns.heatmap(cm_physio, annot=True, fmt='d', cmap='Blues', ax=ax, cbar_kws={'label': 'Count'},\n",
    "            xticklabels=['KEEP', 'INVEST'],\n",
    "            yticklabels=['KEEP', 'INVEST'], \n",
    "            annot_kws={'size': 14, 'weight': 'bold'})\n",
    "ax.set_title(f'Physiology → Outcome\\nAccuracy: {accuracy_physio:.3f} | CV: {cv_scores.mean():.3f}', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# TASK 3: BEHAVIOR → OUTCOME\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"TASK 3: BEHAVIOR → OUTCOME\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Prepare data\n",
    "X_behavior = behavior_df.values\n",
    "y = outcomes\n",
    "\n",
    "# Train-test split (same random state for fair comparison)\n",
    "X_train_behavior, X_test_behavior, y_train, y_test = train_test_split(\n",
    "    X_behavior, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train Random Forest classifier\n",
    "rf_behavior = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    random_state=42, \n",
    "    max_depth=5,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "rf_behavior.fit(X_train_behavior, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_behavior = rf_behavior.predict(X_test_behavior)\n",
    "y_prob_behavior = rf_behavior.predict_proba(X_test_behavior)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "accuracy_behavior = accuracy_score(y_test, y_pred_behavior)\n",
    "print(f\"\\nAccuracy: {accuracy_behavior:.3f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_behavior, target_names=['KEEP', 'INVEST']))\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores_behavior = cross_val_score(rf_behavior, X_behavior, y, cv=5, scoring='accuracy')\n",
    "print(f\"\\n5-Fold Cross-Validation Accuracy: {cv_scores_behavior.mean():.3f} (+/- {cv_scores_behavior.std():.3f})\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance_behavior = pd.DataFrame({\n",
    "    'feature': behavior_df.columns,\n",
    "    'importance': rf_behavior.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nBehavior Feature Importance:\")\n",
    "print(feature_importance_behavior)\n",
    "\n",
    "# Plot feature importance\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.barh(range(len(feature_importance_behavior)), feature_importance_behavior['importance'], \n",
    "        color='seagreen', edgecolor='black')\n",
    "ax.set_yticks(range(len(feature_importance_behavior)))\n",
    "ax.set_yticklabels(feature_importance_behavior['feature'])\n",
    "ax.set_xlabel('Importance', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Behavior Features for Outcome Prediction\\n(All Subjects Combined)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix\n",
    "cm_behavior = confusion_matrix(y_test, y_pred_behavior)\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "sns.heatmap(cm_behavior, annot=True, fmt='d', cmap='Greens', ax=ax, cbar_kws={'label': 'Count'},\n",
    "            xticklabels=['KEEP', 'INVEST'],\n",
    "            yticklabels=['KEEP', 'INVEST'],\n",
    "            annot_kws={'size': 14, 'weight': 'bold'})\n",
    "ax.set_title(f'Behavior → Outcome\\nAccuracy: {accuracy_behavior:.3f} | CV: {cv_scores_behavior.mean():.3f}', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
